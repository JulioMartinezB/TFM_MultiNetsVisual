{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create, modify and work with a general network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymnet import net\n",
    "import numpy as np\n",
    "import random\n",
    "from pymnet import *\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "import heapq\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: No aspects can be added to an empty network, \n",
    "# a 0 aspect network is just a monoplex\n",
    "monoplex = net.MultilayerNetwork(\n",
    "    aspects=0,\n",
    "    noEdge=True,\n",
    "    directed=False,\n",
    "    fullyInterconnected=False,\n",
    ")\n",
    "\n",
    "monoplex.add_node('a')\n",
    "monoplex.add_node('b')\n",
    "monoplex.add_node('c')\n",
    "\n",
    "monoplex['a','b'] = 1\n",
    "monoplex['b','c'] = 2\n",
    "monoplex['c','c'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinetwork = net.MultilayerNetwork(\n",
    "    aspects=1,\n",
    "    noEdge=True,\n",
    "    directed=False,\n",
    "    fullyInterconnected=False,\n",
    ")\n",
    "\n",
    "multinetwork.add_layer('l1', 1)\n",
    "multinetwork.add_layer('l2', 1)\n",
    "\n",
    "multinetwork.add_node('a', 'l1')\n",
    "multinetwork.add_node('c', 'l1')\n",
    "multinetwork.add_node('a', 'l2')\n",
    "multinetwork.add_node('b', 'l2')\n",
    "\n",
    "multinetwork['a','b','l1','l2'] = 1\n",
    "multinetwork['b','c','l2','l1'] = 2\n",
    "multinetwork['a','a','l1','l2'] = 3\n",
    "multinetwork['c','a','l1','l1'] = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. ---Tensorflow functionality---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init_from_sparse_tensor__(sparse_tensor,  nodes_names=None, aspects_names=None,directed=False):\n",
    "    \"\"\"Initialize a MultilayerNetwork from a sparse tensor.\n",
    "    Parameters\n",
    "    ----------\n",
    "    sparse_tensor : tf.SparseTensor\n",
    "        The sparse tensor to initialize the network from.  \n",
    "            Given a sparse tensor of shape (a1,a2,b1,b2,...,an1,an2,n1,n2),\n",
    "            the first n dimensions are the aspects, and the last two dimensions\n",
    "            are the nodes. The values of the sparse tensor are the weights of \n",
    "    nodes_names : list of str\n",
    "        The names of the nodes in the network.\n",
    "    aspects_names : list of list of str\n",
    "        For each aspect, the names of the layers in the aspect.\n",
    "    directed : bool\n",
    "        Whether the network is directed or not. If True, the network is directed.\n",
    "        If False, the network is undirected. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    if sparse_tensor.shape.rank < 2:\n",
    "        raise ValueError(\"The sparse tensor must have at least 2 dimensions.\")\n",
    "    \n",
    "    if len(sparse_tensor.shape[:-2]) % 2 != 0:\n",
    "        raise ValueError(\"The sparse tensor must have an even number of dimensions.\")\n",
    "        \n",
    "    if any([sparse_tensor.shape[i] != sparse_tensor.shape[i+1] for i in range(0, sparse_tensor.shape.rank-2, 2)]):\n",
    "        raise ValueError(\"The sparse tensor must have the same size in the first and second dimensions for each aspect.\")\n",
    "    \n",
    "    if nodes_names and len(nodes_names) != sparse_tensor.shape[-2]:\n",
    "        raise ValueError(\"The number of nodes must be equal to the size of the last two dimensions in the sparse tensor.\")\n",
    "    \n",
    "    if nodes_names is None:\n",
    "        nodes_names = [f\"n{i}\" for i in range(sparse_tensor.shape[-2])]\n",
    "\n",
    "    if aspects_names:\n",
    "        if len(aspects_names) != sparse_tensor.shape[:-2].rank/2:\n",
    "            raise ValueError(\"The number of aspects must be equal to the number of non-nodes dimensions in the sparse tensor: \", sparse_tensor.shape[:-2])\n",
    "    \n",
    "        for i in range(len(aspects_names)):\n",
    "            if len(aspects_names[i]) != sparse_tensor.shape[2*i] or len(aspects_names[i]) != sparse_tensor.shape[2*i+1]:\n",
    "                raise ValueError(f\"The number of layers in aspect {i} must be equal to the size of dimension {i} in the sparse tensor.\")\n",
    "    else:\n",
    "        aspects_names = [ [f'l{i}_{j}' for j in range(sparse_tensor.shape[2*i])] for i in range(len(sparse_tensor.shape[:-2])//2) ]\n",
    "    \n",
    "\n",
    "    # Initialize the network\n",
    "    n = MultilayerNetwork(\n",
    "        aspects=len(aspects_names),\n",
    "        noEdge=False,\n",
    "        directed=directed,\n",
    "        fullyInterconnected=True,\n",
    "    )\n",
    "\n",
    "    for node in nodes_names:\n",
    "        n.add_node(node)\n",
    "\n",
    "    for i in range(len(aspects_names)):\n",
    "        for j in range(len(aspects_names[i])):\n",
    "            n.add_layer(aspects_names[i][j], i+1)\n",
    "\n",
    "    for (index, value) in zip(sparse_tensor.indices, sparse_tensor.values):\n",
    "\n",
    "        aspects_index = index[:-2]\n",
    "        if aspects_index is not None:\n",
    "            edge = [nodes_names[index[-2]],  nodes_names[index[-1]]] + [aspects_names[i//2][aspects_index[i]] for i in range(len(aspects_index))]\n",
    "        else:\n",
    "            edge = [nodes_names[index[-2]],  nodes_names[index[-1]]]\n",
    "        print(edge)\n",
    "        n[tuple(edge)] = value\n",
    "\n",
    "    return n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_sparse_tensor__(self, return_mappings=False):\n",
    "    \"\"\"Get the sparse tensor representation of the network.\n",
    "    Returns\n",
    "    -------\n",
    "    sparse_tensor : tf.SparseTensor\n",
    "        The sparse tensor representation of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'sparse_tensor' in self.__dict__ and self.sparse_tensor is not None:\n",
    "        return self.sparse_tensor\n",
    "    # definde dictionary with name and position of each node\n",
    "    nodes = {}\n",
    "    for i in range(len(self.slices[0])):\n",
    "        nodes[list(self.slices[0])[i]] = i\n",
    "\n",
    "        \n",
    "    layers = [ {list(self.slices[i])[j] : j for j in range(len(self.slices[i]))} for i in range(1, self.aspects+1)]\n",
    "    \n",
    "    indices = []\n",
    "    values = []\n",
    "    for edge in self.edges:\n",
    "        edge_nodes = edge[:2]\n",
    "        edge_aspects = edge[2:-1]\n",
    "        \n",
    "        edge_aspects = [layers[i//2][edge_aspects[i]] for i in range(len(edge_aspects))]\n",
    "        edge_nodes = [nodes[edge_nodes[i]] for i in range(len(edge_nodes))]\n",
    "\n",
    "        indices.append(edge_nodes + edge_aspects)\n",
    "        values.append(edge[-1])\n",
    "\n",
    "        if not self.directed:\n",
    "            inverted_edge = edge_nodes[::-1] + edge_aspects[::-1]\n",
    "            indices.append(inverted_edge)\n",
    "            values.append(edge[-1])\n",
    "    \n",
    "        \n",
    "\n",
    "    indices = tf.constant(indices, dtype=tf.int64)\n",
    "    values = tf.constant(values, dtype=tf.float32)\n",
    "    \n",
    "    # shape = [len(self.slices[i]) for i in range(self.aspects+1)]\n",
    "    shape = [len(self.slices[i//2]) for i in range(0,(self.aspects+1)*2)]\n",
    "\n",
    "    \n",
    "    sparse_tensor = tf.SparseTensor(indices=indices, values=values, dense_shape=shape)\n",
    "    sparse_tensor = tf.sparse.reorder(sparse_tensor)  # Ensure the indices are sorted\n",
    "     \n",
    "    if return_mappings:\n",
    "        inv_nodes = {v: k for k, v in nodes.items()}\n",
    "        inv_layers = [{v: k for k, v in l.items()} for l in layers]\n",
    "        return sparse_tensor, nodes, layers, inv_nodes, inv_layers\n",
    "    \n",
    "    return sparse_tensor\n",
    "\n",
    "\n",
    "def __get_sparse_tensor_between_layers__(self,l1,l2):\n",
    "    \"\"\"\n",
    "    Get the sparse tensor representation of the network between two layers.\n",
    "    \n",
    "    Args:\n",
    "        l1 (list of str/int) : The names of the first layer.\n",
    "        l2 (list of str/int) : The names of the second layer. \n",
    "        \n",
    "    Returns:\n",
    "        tf.sparse.SparseTensor: The sparse tensor representation of the network between the two layers.\n",
    "    \"\"\"\n",
    "    #generate a list fix_index = [l1[0], l2[0], l1[1], l2[1], ..., l1[n-1], l2[n-1]]\n",
    "\n",
    "    \n",
    "    sparse, nodes, layers, inv_nodes, inv_layers = __get_sparse_tensor__(self, return_mappings=True)\n",
    "    for i in range(len(l1)):\n",
    "        if isinstance(l1[i], str):\n",
    "            l1[i] = layers[0][l1[i]]\n",
    "        if isinstance(l2[i], str):\n",
    "            l2[i] = layers[0][l2[i]]\n",
    "\n",
    "    fixed_indices = [item for pair in zip(l1, l2) for item in pair]\n",
    "\n",
    "    # fixed_indices = []\n",
    "    # for i in range(len(l1)):\n",
    "    #     fixed_indices.append(l1[i])\n",
    "    #     fixed_indices.append(l2[i])\n",
    "    \n",
    "\n",
    "    return __slice_sparse_tensor__(sparse, fixed_indices)\n",
    "\n",
    "def __slice_sparse_tensor__(sparse_tensor, fixed_indices):\n",
    "    \"\"\"\n",
    "    Extracts a slice from a 2k-dimensional SparseTensor by fixing the last k dimensions.\n",
    "    \n",
    "    Args:\n",
    "        sparse_tensor (tf.sparse.SparseTensor): The original sparse tensor of shape [d1, d2, ..., d_{2k}]\n",
    "        fixed_indices (List[int]): A list of length k containing fixed indices for the last k dimensions.\n",
    "        \n",
    "    Returns:\n",
    "        tf.sparse.SparseTensor: A sparse tensor of shape [d1, ..., d_k] corresponding to the slice.\n",
    "    \"\"\"\n",
    "    \n",
    "    fixed_indices = tf.convert_to_tensor(fixed_indices, dtype=tf.int64)\n",
    "    num_dims = tf.shape(sparse_tensor.dense_shape)[0]\n",
    "    k = tf.size(fixed_indices)\n",
    "\n",
    "    \n",
    "    # Split indices into first k and last k parts\n",
    "    first_k = sparse_tensor.indices[:, :num_dims - k]\n",
    "    last_k = sparse_tensor.indices[:, num_dims - k:]\n",
    "    \n",
    "    # Create mask for matching fixed_indices (login and between last_k and fixed_indices)\n",
    "    mask = tf.reduce_all(tf.equal(last_k, fixed_indices), axis=1)\n",
    "\n",
    "    \n",
    "    # Filter indices and values\n",
    "    new_indices = tf.boolean_mask(first_k, mask)\n",
    "    new_values = tf.boolean_mask(sparse_tensor.values, mask)\n",
    "    \n",
    "    # New shape is the first k dimensions\n",
    "    new_shape = sparse_tensor.dense_shape[:num_dims - k]\n",
    "    \n",
    "    return tf.SparseTensor(indices=new_indices, values=new_values, dense_shape=new_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. ---Tensroflow generation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er_general_multilayer(\n",
    "    n_layers_per_aspect : list,\n",
    "    n_nodes : int,\n",
    "    p=0.5,\n",
    "    randomWeights=False,\n",
    "    directed=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a random multilayer network with the given parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers_per_aspect : list of int\n",
    "        The number of layers per aspect. Default is [2,2].\n",
    "    n_nodes : int\n",
    "        The number of nodes in the network. Default is 8.\n",
    "    p : float\n",
    "        The probability of creating an edge between two nodes. Default is 0.5.\n",
    "    randomWeights : bool\n",
    "        Whether to use random weights for the edges or not. Default is False.\n",
    "    directed : bool\n",
    "        Whether the network is directed or not. Default is False.\n",
    "    fullyInterconnected : bool\n",
    "        Determines if the network is fully interconnected, i.e. all nodes\n",
    "       are shared between all layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    net : MultilayerNetwork\n",
    "        The generated multilayer network.\n",
    "    \n",
    "    \"\"\"\n",
    "    mnet = MultilayerNetwork(\n",
    "        aspects=len(n_layers_per_aspect),\n",
    "        directed=directed\n",
    "    )\n",
    "\n",
    "    for node in range(n_nodes):\n",
    "        mnet.add_node(node)\n",
    "    if not n_layers_per_aspect:\n",
    "        for aspect in range(len(n_layers_per_aspect)):\n",
    "            for layer in range(n_layers_per_aspect[aspect]):\n",
    "                mnet.add_layer(layer, aspect+1)\n",
    "    \n",
    "    for edge in __generate_random_edges_er(n_nodes, n_layers_per_aspect, p):\n",
    "        if randomWeights:\n",
    "            weight = random.random()\n",
    "        else:\n",
    "            weight = 1\n",
    "        mnet[tuple(edge)] = weight\n",
    "\n",
    "    \n",
    "    return mnet\n",
    "\n",
    "def __generate_random_edges_er(n_nodes, aspect_sizes, p, directed=False):\n",
    "    \"\"\"\n",
    "    Efficient generator of random edges in a multilayer graph.\n",
    "\n",
    "    Each node-layer is a tuple of integers: (node, layer1, ..., layerN)\n",
    "\n",
    "    Parameters:\n",
    "        n_nodes (int): Number of nodes.\n",
    "        aspect_sizes (list of int): Number of layers for each aspect.\n",
    "        p (float): Probability of including an edge between two node-layers.\n",
    "\n",
    "    Yields:\n",
    "        tuple: An edge as a tuple of two node-layer tuples.\n",
    "    \"\"\"\n",
    "    if len(aspect_sizes) > 0:\n",
    "        total_layers = 1\n",
    "        for size in aspect_sizes:\n",
    "            total_layers *= size\n",
    "        total_node_layers = n_nodes * total_layers\n",
    "\n",
    "        # Helper: convert flat index to node-layer tuple\n",
    "        def index_to_node_layer(idx):\n",
    "            node = idx // total_layers\n",
    "            rest = idx % total_layers\n",
    "            layers = []\n",
    "            for size in reversed(aspect_sizes):\n",
    "                layers.append(rest % size)\n",
    "                rest //= size\n",
    "            return (node, *reversed(layers))\n",
    "\n",
    "        for i in range(total_node_layers):\n",
    "            for j in range(i + 1, total_node_layers):\n",
    "                if random.random() < p:\n",
    "                    nl1 = index_to_node_layer(i)\n",
    "                    nl2 = index_to_node_layer(j)\n",
    "                    yield (nl1[0], nl2[0], *nl1[1:], *nl2[1:])  # Flatten the tuple\n",
    "    else:\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i + 1, n_nodes):\n",
    "                if random.random() < p:\n",
    "                    yield (i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ws_multiplex(\n",
    "        n_nodes,\n",
    "        n_layers,\n",
    "        degrees = [],\n",
    "        rewiring_probs = [],\n",
    ") :\n",
    "    \"\"\"Generate multilayer Watts-Strogatz network.\n",
    "\n",
    "    The produced multilayer network has a single aspect.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes : int\n",
    "        Number of nodes\n",
    "    n_layers : int\n",
    "        Number of layers\n",
    "    degrees : list\n",
    "        List of degrees for each layer\n",
    "    p_rewiring : list\n",
    "        List of rewiring probabilities for each layer\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    multplex = MultiplexNetwork(couplings='categorical')\n",
    "    rewires = [] \n",
    "    for i in range(n_nodes):\n",
    "        multplex.add_node(i)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        multplex.add_layer(i)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        deg = degrees[i]//2\n",
    "        for j in range(n_nodes):\n",
    "            for edges in range(deg):\n",
    "                multplex.A[i][j,(j+edges+1)%n_nodes] = 1\n",
    "                \n",
    "    for i in range(n_layers):\n",
    "        for j in range(n_nodes):\n",
    "                # iterate on neighbors of node j in layer i\n",
    "                neighbors = list(multplex._iter_neighbors_out((j,i),(None,i)))\n",
    "                not_neighbors = _get_not_neighbors_layer(multplex, j, i)\n",
    "\n",
    "                for neighbor in neighbors:\n",
    "                    if random.random() < rewiring_probs[i] and len(not_neighbors) > 0:\n",
    "                        rewired = random.choice(not_neighbors)\n",
    "                        print(((j,i), neighbor), \" to \", ((j,i), (rewired,i)))\n",
    "                        multplex.A[i][j,neighbor[0]] = 0\n",
    "                        multplex.A[i][j,rewired] = 1\n",
    "                        rewire = ((j,i),(neighbor,i))\n",
    "                        rewires.append(rewire)\n",
    "                        \n",
    "    \n",
    "    return multplex, rewires\n",
    "\n",
    "def _get_not_neighbors_layer(net, node, layer, loops = False):\n",
    "    nodelayers = [(x,layer)  for x in list(net.iter_nodes(layer))]\n",
    "    neighbors = list(net._iter_neighbors((node, layer),(None,layer)))\n",
    "    not_neighbors = [node for node in nodelayers if node not in neighbors]\n",
    "    if not loops:\n",
    "        not_neighbors = [n[0] for n in not_neighbors if n != (node,layer)]\n",
    "    return not_neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_multilayer_edges_streaming(n_nodes, aspects, p):\n",
    "    \"\"\"\n",
    "    Generate random edges for a multilayer graph with minimal memory usage.\n",
    "    Each edge connects two different node-layers with probability p.\n",
    "\n",
    "    Parameters:\n",
    "        n_nodes (list): List of node identifiers.\n",
    "        aspects (list of lists): List of aspects, each a list of possible values.\n",
    "        p (float): Probability of including an edge.\n",
    "\n",
    "    Yields:\n",
    "        tuple: An edge as a tuple of two node-layer tuples.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    total_layers = 1\n",
    "    for a in aspects:\n",
    "        total_layers *= len(a)\n",
    "    total_nodes = len(n_nodes) * total_layers\n",
    "\n",
    "    # Use indexed access to avoid building the list\n",
    "    def node_layer_at(index):\n",
    "        node_index = index // total_layers\n",
    "        layer_index = index % total_layers\n",
    "        layer_values = []\n",
    "        for a in reversed(aspects):\n",
    "            layer_values.append(a[layer_index % len(a)])\n",
    "            layer_index //= len(a)\n",
    "        return (n_nodes[node_index], *reversed(layer_values))\n",
    "\n",
    "    # Iterate through pairs by index (i < j) to avoid storing all pairs\n",
    "    for i in range(total_nodes):\n",
    "        for j in range(i + 1, total_nodes):\n",
    "            if random.random() < p:\n",
    "                yield (node_layer_at(i), node_layer_at(j)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. ---Centrality measures--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        PCI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(network, start, max_distance = 100000):\n",
    "    \"\"\"\n",
    "    Given a multilayer network and a node of the network,\n",
    "    this function returns a dictionary with the distances from the\n",
    "    node to all the other nodes in the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : MultilayerNetwork\n",
    "        The multilayer network to analyze\n",
    "    start : tuple\n",
    "        The node to analyze\n",
    "    max_distance : int\n",
    "        The maximum distance to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distances : dict\n",
    "        A dictionary with the distances from the node to all the other nodes\n",
    "    \"\"\"\n",
    "    prio_queue = [(0,start)]  \n",
    "    distances = { start : 0}\n",
    "    visited = set()\n",
    "\n",
    "    while prio_queue:\n",
    "        distance, node = heapq.heappop(prio_queue)\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        for neighbor in network._iter_neighbors(node):\n",
    "            if neighbor not in visited and distance+1 <= max_distance and neighbor not in distances:\n",
    "                heapq.heappush(prio_queue, (distance+1,neighbor))\n",
    "                distances[neighbor] = distance+ network[network._nodes_to_link(node, neighbor)]\n",
    "    \n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_connections(net, node):\n",
    "    \"\"\"\n",
    "    Given a multilayer network and a node of the network,\n",
    "    this function returns a dictionary with the number of connections\n",
    "    of the node to each layer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : MultilayerNetwork\n",
    "        The multilayer network to analyze\n",
    "    node : tuple\n",
    "        The node to analyze\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    connections : dict\n",
    "        A dictionary with the number of connections of the node to each layer\n",
    "    \"\"\"\n",
    "    connections = {}\n",
    "    for neighbor in net._iter_neighbors(node):\n",
    "        layer = neighbor[1:]\n",
    "        if layer in connections:\n",
    "            connections[layer] += 1\n",
    "        else:\n",
    "            connections[layer] = 1\n",
    "    return connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the PCI functions\n",
    "\n",
    "\n",
    "def mu_PCI(net, node, mu = 1):\n",
    "    \"\"\"Given a multilayer network and a node of the network,\n",
    "        this function returns the \\mu-PCI of the node. The mu-PCI\n",
    "        of a node is the maximum number k of nodes that are at most\n",
    "        mu hops away from the node and have a degree of at least k.\n",
    "\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        net : MultilayerNetwork\n",
    "            The multilayer network to analyze\n",
    "        node : tuple\n",
    "            The node to analyze\n",
    "        mu : float\n",
    "            The mu maximum distance to consider\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu_PCI : int\n",
    "            The \\mu-PCI of the node  \n",
    "\n",
    "    \"\"\"\n",
    "    distances = dijkstra(net, node, mu)\n",
    "    # remove node from distances\n",
    "    distances.pop(node, None)\n",
    "    degrees = np.array([net._get_degree(node) for node in distances])\n",
    "    # sort the degrees descending\n",
    "    degrees = np.sort(degrees)[::-1]\n",
    "\n",
    "    # indices = np.arange(1,len(degrees)+1)\n",
    "    seq = (k for k, d in enumerate(degrees, start=1) if d >= k)\n",
    "    if not seq:\n",
    "        mu_PCI = 0\n",
    "    else:\n",
    "        mu_PCI = max(seq, default=0)\n",
    "    return mu_PCI\n",
    "        \n",
    "\n",
    "\n",
    "def mlPCI_n(net,node,n = 1):\n",
    "    \"\"\"\n",
    "    Given a multilayer network and a node of the network,\n",
    "    this function returns the ml-PCI_n of the node. The ml-PCI_n is the \n",
    "    maximum number k of neighbors of the node that are connected to at least\n",
    "    n different layers with k edges.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze\n",
    "    node : tuple\n",
    "        The node to analyze\n",
    "    n : int\n",
    "        The number of layers to consider\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mlPCI_n : int\n",
    "        The ml-PCI_n of the node    \n",
    "    \"\"\"\n",
    "    #layer_degrees is a numpy array with k rows and n columns, being k the number of neighbors of the node and n the parameter n\n",
    "    layer_degrees = np.zeros(len([1 for _ in net._iter_neighbors(node)]))\n",
    "    # for each neighbor of the node, get the number of connections to each layer and store it in layer_degrees\n",
    "    for i, neighbor in enumerate(net._iter_neighbors(node)):\n",
    "        layer_conn = layer_connections(net, neighbor)\n",
    "        layer_conn = sorted(list(layer_conn.values()),reverse=True)\n",
    "        if len(layer_conn) < n:\n",
    "            layer_degrees[i] = 0\n",
    "        else:\n",
    "            ey = layer_conn[n-1]\n",
    "            layer_degrees[i] = ey\n",
    "        \n",
    "    layer_degrees = sorted(layer_degrees,reverse=True)\n",
    "    \n",
    "    # mlPCI_n = np.max(np.where(layer_degrees >= indices, indices, 0))\n",
    "    seq = (k for k, d in enumerate(layer_degrees, start=1) if d >= k)\n",
    "    # if seq is empty, return 0\n",
    "    if not seq:\n",
    "        mlPCI_n = 0\n",
    "    else:\n",
    "    # get the maximum k such that d >= k\n",
    "        mlPCI_n = max(seq, default=0)\n",
    "\n",
    "\n",
    "    return mlPCI_n\n",
    "                    \n",
    "   \n",
    "def allPCI(net,node):\n",
    "    \"\"\"\n",
    "    Given a multilayer network and a node of the network,\n",
    "    this function returns the all-PCI of the node. The all-PCI is the\n",
    "    maximum number k of neighbors of the node that are connected\n",
    "    to all the layers of the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze\n",
    "    node : tuple\n",
    "        The node to analyze\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_PCI : int\n",
    "        The all-PCI of the node\n",
    "    \"\"\"\n",
    "    aspects_lens = np.array([len(net.slices[i]) for i in range(1,net.aspects+1)])\n",
    "    prod = aspects_lens.prod()\n",
    "\n",
    "    all_PCI =mlPCI_n(net,node,prod)\n",
    "    return all_PCI \n",
    "\n",
    "def lsPCI(net,node):\n",
    "    \"\"\"\n",
    "    Given a multilayer network and a node of the network,\n",
    "    this function returns the ls-PCI of the node. The ls-PCI is the\n",
    "    maximum number k of neighbors of the node that are connected to\n",
    "    at least k different nodes of at least k different layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze\n",
    "    node : tuple\n",
    "        The node to analyze\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ls_PCI : int\n",
    "        The ls-PCI of the node\n",
    "    \"\"\"\n",
    "    neighbors_layer_degrees = []\n",
    "    for neighbor in net._iter_neighbors(node):\n",
    "        layer_conn = layer_connections(net, neighbor)\n",
    "        neighbors_layer_degrees.append(sorted(list(layer_conn.values()),reverse=True))\n",
    "    dim2_seq = [len(layer_conn) for layer_conn in neighbors_layer_degrees]\n",
    "    if len(dim2_seq) == 0:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "    nl_matrix = np.zeros((len(neighbors_layer_degrees),max([len(layer_conn) for layer_conn in neighbors_layer_degrees])))\n",
    "    for i, layer_conn in enumerate(neighbors_layer_degrees):\n",
    "        nl_matrix[i,:len(layer_conn)] = layer_conn\n",
    "\n",
    "    lsPCI = 0\n",
    "    for i in range(0,nl_matrix.shape[1]):\n",
    "        discarded = np.where(nl_matrix[:,i] < i)[0]\n",
    "        if len(discarded) > i:\n",
    "            lsPCI = i\n",
    "            break\n",
    "    return lsPCI\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigencentrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_power_iteration(sparse_matrix, num_iters=1000):\n",
    "    n = sparse_matrix.dense_shape[1]\n",
    "    sparse_matrix = tf.sparse.reorder(sparse_matrix)  # ensure canonical order\n",
    "    b_k = tf.random.normal([n, 1])  # column vector\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "\n",
    "        b_k1 = tf.sparse.sparse_dense_matmul(sparse_matrix, b_k)\n",
    "        norm = tf.norm(b_k1)\n",
    "        b_k = b_k1 / norm\n",
    "\n",
    "    return tf.squeeze(b_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def independent_layer_eigenvector_centrality(self):\n",
    "    # if tyupe of layers is not int, map them to int\n",
    "    layers_to_int = {layer: i for i, layer in enumerate(self.get_layers())}\n",
    "    int_to_layers = {i: layer for i, layer in enumerate(self.get_layers())}\n",
    "    layers = [layers_to_int[layer] for layer in self.get_layers()]\n",
    "    layers = tf.convert_to_tensor(layers, dtype=tf.int32)\n",
    "    # layers = tf.convert_to_tensor(list(self.get_layers()), dtype=tf.int32)\n",
    "\n",
    "    def map_fn_body(li):\n",
    "        # li is a scalar tensor (layer index)\n",
    "        layer_matrix = __get_sparse_tensor_between_layers__(self, tf.expand_dims(li, 0), tf.expand_dims(li, 0))\n",
    "        layer_matrix = tf.sparse.reorder(layer_matrix)\n",
    "        eigvec = sparse_power_iteration(layer_matrix)  # returns [n,] or [n,1]\n",
    "        return eigvec\n",
    "\n",
    "    eigenvectors = tf.map_fn(map_fn_body, layers, fn_output_signature=tf.TensorSpec([None], tf.float32))\n",
    "    return eigenvectors, int_to_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_eigenvector_centrality(self):\n",
    "    \"\"\"\n",
    "    Compute the eigenvector centrality for each layer in a uniform manner.\n",
    "    \n",
    "    This function computes the eigenvector centrality for the aggregated adjacency matrix of all layers in the network.\n",
    "    It sums the adjacency matrices of all layers and then applies the power iteration method to compute the eigenvector centrality.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        A tensor containing the eigenvector centrality for each layer.\n",
    "    \"\"\"\n",
    "    layers = self.get_layers()\n",
    "\n",
    "    sum_tensor = None\n",
    "\n",
    "    for li in layers:\n",
    "        # Get the sparse tensor for the current layer\n",
    "        layer_matrix = __get_sparse_tensor_between_layers__(self, [li], [li])\n",
    "        layer_matrix = tf.sparse.reorder(layer_matrix)  # ensure canonical order\n",
    "        \n",
    "        if sum_tensor is None:\n",
    "            sum_tensor = layer_matrix\n",
    "        else:\n",
    "            sum_tensor = tf.sparse.add(sum_tensor, layer_matrix)\n",
    "\n",
    "\n",
    "    # Compute the eigenvector centrality for the aggregated layer matrix\n",
    "    eigvec = sparse_power_iteration(sum_tensor)\n",
    "    return eigvec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_heterogeneus_eigenvector_centrality(self:MultilayerNetwork, weights = None ):\n",
    "    #weights must be a [n_layers,n_layers] dense tensor, where n_layers is the number of layers in the network\n",
    "    n_layers = len(self.get_layers())\n",
    "    tensor, nodes_to_int, layers_to_int, int_to_nodes, int_to_layers = __get_sparse_tensor__(self, return_mappings=True)\n",
    "    \n",
    "    if (weights is not None) and (weights.shape[0] != n_layers or weights.shape[1] != n_layers):\n",
    "        raise ValueError(f\"Weights must be a square tensor of shape [{n_layers}, {n_layers}]. Got {weights.shape}.\")\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = tf.constant(np.identity(n_layers), dtype=tf.float32)\n",
    "\n",
    "    A_star = __sparse_masked_tensordot(tensor, weights)\n",
    "\n",
    "    A_star = tf.sparse.reorder(A_star)  # Ensure the sparse tensor is in canonical order\n",
    "    \n",
    "    layers_tensor = tf.convert_to_tensor(list(range(n_layers)), dtype=tf.int32)\n",
    "    # layers = tf.convert_to_tensor(list(self.get_layers()), dtype=tf.int32)\n",
    "\n",
    "    def compute_eigvec(li):\n",
    "        layer_matrix = __slice_sparse_tensor__(A_star, [li,li])\n",
    "        layer_matrix = tf.sparse.reorder(layer_matrix)\n",
    "        eigvec = sparse_power_iteration(layer_matrix)\n",
    "        return eigvec\n",
    "    \n",
    "    eigenvectors = tf.map_fn(compute_eigvec, layers_tensor, fn_output_signature=tf.TensorSpec([None], tf.float32))\n",
    "    return eigenvectors, nodes_to_int, layers_to_int, int_to_nodes, int_to_layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def __sparse_masked_tensordot(A_sparse: tf.SparseTensor, W: tf.Tensor) -> tf.SparseTensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A_sparse: tf.SparseTensor of shape [n, n, d, d]\n",
    "        W: Tensor of shape [d, d], weights for combining diagonals\n",
    "    \n",
    "    Returns:\n",
    "        A_sparse_star: tf.SparseTensor of shape [n, n, d, d], where only entries at [:,:,k,k]\n",
    "                       are non-zero, and:\n",
    "            A_star[i,j,k,k] = sum_{t=k}^{d-1} W[k, t] * A[i,j,t,t]\n",
    "    \"\"\"\n",
    "    A_sparse = tf.sparse.reorder(A_sparse)\n",
    "    indices = A_sparse.indices     # [nnz, 4]\n",
    "    values = A_sparse.values       # [nnz]\n",
    "    dense_shape = A_sparse.dense_shape\n",
    "    n, _, d1, d2 = dense_shape.numpy()\n",
    "    assert d1 == d2, \"Expected square last dimensions for diagonals.\"\n",
    "\n",
    "    # Extract only diagonal entries: where indices[:,2] == indices[:,3]\n",
    "    is_diag = tf.equal(indices[:, 2], indices[:, 3])\n",
    "    diag_indices = tf.boolean_mask(indices, is_diag)\n",
    "    diag_values = tf.boolean_mask(values, is_diag)\n",
    "    diag_t = diag_indices[:, 2]  # the shared diagonal index t\n",
    "\n",
    "    output_entries = {}  # (i, j, k) -> value\n",
    "\n",
    "    for k in range(d1):\n",
    "        # Select entries with t >= k\n",
    "        mask_k = tf.greater_equal(diag_t, k)\n",
    "        selected_indices = tf.boolean_mask(diag_indices, mask_k)\n",
    "        selected_values = tf.boolean_mask(diag_values, mask_k)\n",
    "        selected_t = tf.boolean_mask(diag_t, mask_k)\n",
    "\n",
    "        # Apply weights: W[k, t]\n",
    "        weights = tf.gather(W[k], selected_t)\n",
    "        weighted_vals = selected_values * weights\n",
    "\n",
    "        for idx, val in zip(selected_indices.numpy(), weighted_vals.numpy()):\n",
    "            i, j, t, _ = idx\n",
    "            key = (i, j, k, k)\n",
    "            output_entries[key] = output_entries.get(key, 0.0) + val\n",
    "\n",
    "    final_indices = tf.constant(list(output_entries.keys()), dtype=tf.int64)\n",
    "    final_values = tf.constant(list(output_entries.values()), dtype=tf.float32)\n",
    "\n",
    "    return tf.SparseTensor(indices=final_indices, values=final_values, dense_shape=dense_shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khatri_rao_weighted_block_sparse(W: tf.Tensor, A_sparse: tf.SparseTensor) -> tf.SparseTensor:\n",
    "    \"\"\"\n",
    "    Compute the Khatri-Rao block product of a sparse tensor:\n",
    "        A^⊗[i,j] = W[i,j] * A_j, where A_j = A_sparse[:,:,j,j]\n",
    "\n",
    "    Args:\n",
    "        W: Tensor of shape [m, m]\n",
    "        A_sparse: SparseTensor of shape [n, n, m, m] (assumes A_j = A[:,:,j,j])\n",
    "\n",
    "    Returns:\n",
    "        A dense tensor of shape [n*m, n*m]\n",
    "    \"\"\"\n",
    "    n = tf.cast(A_sparse.dense_shape[0], tf.int32)\n",
    "    m = tf.shape(W)[0]\n",
    "    \n",
    "    indices = A_sparse.indices\n",
    "    values = A_sparse.values\n",
    "\n",
    "    # Only keep diagonal slices: i == j in the last two dims\n",
    "    diag_mask = tf.equal(indices[:, 2], indices[:, 3])\n",
    "    diag_indices = tf.boolean_mask(indices, diag_mask)\n",
    "    diag_values = tf.boolean_mask(values, diag_mask)\n",
    "\n",
    "    output_indices = []\n",
    "    output_values = []\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            # Extract entries of A_j = A[:, :, j, j]\n",
    "            mask_j = tf.equal(diag_indices[:, 2], j)\n",
    "            indices_j = tf.boolean_mask(diag_indices, mask_j)[:, :2]  # [row, col]\n",
    "            values_j = tf.boolean_mask(diag_values, mask_j)\n",
    "\n",
    "            # Compute block offset\n",
    "            row_offset = i * n\n",
    "            col_offset = j * n\n",
    "            \n",
    "            # Shift indices into block [i,j]\n",
    "            shifted_indices = indices_j + tf.cast(tf.stack([row_offset, col_offset]), tf.int64)\n",
    "\n",
    "            # Scale values by W[i, j]\n",
    "            scaled_values = tf.cast(W[i, j], tf.float32) * values_j\n",
    "\n",
    "            output_indices.append(shifted_indices)\n",
    "            output_values.append(scaled_values)\n",
    "\n",
    "    all_indices = tf.concat(output_indices, axis=0)\n",
    "    all_values = tf.concat(output_values, axis=0)\n",
    "    dense_shape = tf.constant(np.array([n*m, n*m]), dtype=tf.int64)\n",
    "\n",
    "    return tf.SparseTensor(indices=all_indices, values=all_values, dense_shape=dense_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_heterogeneus_eigenvector_centrality(self:MultilayerNetwork, weights = None):\n",
    "    \"\"\"\n",
    "    Compute the global eigenvector centrality for a multilayer network with heterogeneous weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    self : MultilayerNetwork\n",
    "        The multilayer network to compute the eigenvector centrality for.\n",
    "    weights : tf.Tensor, optional\n",
    "        A square tensor of shape [n_layers, n_layers] representing the weights for each layer pair.\n",
    "        If None, an identity matrix is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        A tensor containing the global eigenvector centrality for each layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_layers = len(self.get_layers())\n",
    "    \n",
    "    if (weights is not None) and (weights.shape[0] != n_layers or weights.shape[1] != n_layers):\n",
    "        raise ValueError(f\"Weights must be a square tensor of shape [{n_layers}, {n_layers}]. Got {weights.shape}.\")\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = tf.identity(np.identity(n_layers))\n",
    "\n",
    "    sparse_tensor, nodes_to_int, layers_to_int, int_to_nodes, int_to_layers = __get_sparse_tensor__(self, return_mappings=True)\n",
    "    \n",
    "    A_block = khatri_rao_weighted_block_sparse(weights, sparse_tensor)\n",
    "\n",
    "    A_block = tf.sparse.reorder(A_block)  # Ensure the sparse tensor is in canonical order\n",
    "    \n",
    "    eigvec = sparse_power_iteration(A_block)\n",
    "    #divide eigvec in n_layers parts of size n_nodes\n",
    "    n_nodes = len(self.slices[0]) \n",
    "    eigvec = tf.reshape(eigvec, (n_layers,n_nodes))\n",
    "    \n",
    "    return eigvec, nodes_to_int, layers_to_int, int_to_nodes, int_to_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. ---Deteccion Comunidades---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinet_coms = net.MultiplexNetwork(\n",
    "    couplings='categorical',\n",
    ")\n",
    "multinet_coms.add_layer(1)\n",
    "multinet_coms.add_layer(2)\n",
    "multinet_coms.add_layer(3)\n",
    "\n",
    "multinet_coms.add_node(1)\n",
    "multinet_coms.add_node(2)\n",
    "multinet_coms.add_node(3)\n",
    "multinet_coms.add_node(4)\n",
    "multinet_coms.add_node(5)\n",
    "multinet_coms.add_node(6)\n",
    "\n",
    "multinet_coms[(1,1)][(3,1)] = 1\n",
    "# multinet_coms[(1,1)][(2,1)] = 1\n",
    "multinet_coms[(1,1)][(4,1)] = 1\n",
    "multinet_coms[(1,1)][(5,1)] = 1\n",
    "multinet_coms[(2,1)][(4,1)] = 2\n",
    "multinet_coms[(4,1)][(6,1)] = 1\n",
    "multinet_coms[(4,1)][(3,1)] = 1\n",
    "multinet_coms[(5,1)][(6,1)] = 1\n",
    "\n",
    "\n",
    "multinet_coms[(2,2)][(3,2)] = 1\n",
    "multinet_coms[(2,2)][(4,2)] = 2\n",
    "# multinet_coms[(2,2)][(6,2)] = 1\n",
    "multinet_coms[(4,2)][(3,2)] = 1\n",
    "multinet_coms[(4,2)][(1,2)] = 1\n",
    "multinet_coms[(4,2)][(5,2)] = 1\n",
    "multinet_coms[(5,2)][(3,2)] = 1\n",
    "multinet_coms[(5,2)][(6,2)] = 1\n",
    "\n",
    "# multinet_coms[(1,3)][(2,3)] = 1\n",
    "multinet_coms[(1,3)][(3,3)] = 1\n",
    "multinet_coms[(1,3)][(4,3)] = 1\n",
    "multinet_coms[(2,3)][(3,3)] = 1\n",
    "multinet_coms[(2,3)][(4,3)] = 1\n",
    "multinet_coms[(3,3)][(4,3)] = 1\n",
    "# multinet_coms[(3,3)][(5,3)] = 1\n",
    "# multinet_coms[(4,3)][(6,3)] = 1\n",
    "multinet_coms[(5,3)][(6,3)] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mucha et al. (2010)\tfor new modularity in multiplex networks\n",
    "def modularity(communities):\n",
    "    mod = 0\n",
    "    graph_size = communities.get('graph_size', 0)\n",
    "    for community in communities['com2nodes'].keys():\n",
    "        comm_inner = communities['com_inner_weight'][community]\n",
    "        comm_total = communities['com_total_weight'][community]\n",
    "        mod += comm_inner/(2*graph_size) - (comm_total/(2*graph_size))**2\n",
    "    return mod\n",
    "\n",
    "\n",
    "\n",
    "def merge_coms(coms,com1,com2,weigth_inter_coms):\n",
    "    new_coms = coms.copy()\n",
    "    new_coms['com2nodes'][com1] = new_coms['com2nodes'][com1] + new_coms['com2nodes'][com2]\n",
    "\n",
    "    for node in new_coms['com2nodes'][com2]:\n",
    "        new_coms['node2com'][node] = com1\n",
    "        \n",
    "    # weigth_inter_coms duplicated as those edges become edges between a node and itself  \n",
    "    new_coms['com_inner_weight'][com1] += new_coms['com_inner_weight'][com2] + 2*weigth_inter_coms\n",
    "    new_coms['com_total_weight'][com1] += new_coms['com_total_weight'][com2] \n",
    "\n",
    "    new_coms['com2nodes'].pop(com2)\n",
    "    new_coms['com_inner_weight'].pop(com2)\n",
    "    new_coms['com_total_weight'].pop(com2)\n",
    "\n",
    "    return new_coms\n",
    "\n",
    "def weights_inter_coms(net,coms):\n",
    "    inter_coms = defaultdict(int)\n",
    "    for edge in net.edges:\n",
    "        # last value in edge is the weight, so we ignore it\n",
    "        edge = tuple(list(edge)[:-1])\n",
    "        n1,n2 = net._link_to_nodes(edge)\n",
    "        com1 = coms['node2com'][n1]\n",
    "        com2 = coms['node2com'][n2]\n",
    "        if com1 != com2:\n",
    "            inter_coms[(min(com1, com2), max(com1, com2))] += net[n1][n2]\n",
    "    return inter_coms\n",
    "\n",
    "\n",
    "\n",
    "def louvain_step(net,communities):\n",
    "    com_keys = list(communities['com2nodes'].keys())\n",
    "    n_coms = len(com_keys)\n",
    "    max_objective_value = communities['objective_function_value']\n",
    "    weights = weights_inter_coms(net,communities)\n",
    "    \n",
    "    max_coms = communities\n",
    "\n",
    "    for i in range(n_coms):\n",
    "        for j in range(i+1,n_coms):\n",
    "\n",
    "            c1, c2 = com_keys[i], com_keys[j]\n",
    "            if (c1, c2) not in weights and (c2, c1) not in weights:\n",
    "                continue\n",
    "            w_inter = weights.get((c1, c2), weights.get((c2, c1), 0))\n",
    "\n",
    "            temp_coms = {\n",
    "                'com2nodes': {k: v[:] for k, v in communities['com2nodes'].items()},\n",
    "                'node2com' : communities['node2com'].copy(),\n",
    "                'com_inner_weight': communities['com_inner_weight'].copy(),\n",
    "                'com_total_weight': communities['com_total_weight'].copy(),\n",
    "                'graph_size': communities['graph_size'],\n",
    "                'objective_function': communities['objective_function'],\n",
    "                'objective_function_name': communities['objective_function_name'],\n",
    "                'objective_function_value': communities['objective_function_value'],\n",
    "            }\n",
    "\n",
    "            # print(com_keys[i],com_keys[j])\n",
    "            merged_coms = merge_coms(temp_coms,com_keys[i],com_keys[j],weights[(com_keys[i],com_keys[j])])\n",
    "            new_objective_value = merged_coms['objective_function'](merged_coms)\n",
    "            merged_coms['objective_function_value'] = new_objective_value\n",
    "            \n",
    "            \n",
    "            if new_objective_value > max_objective_value:\n",
    "                max_objective_value = new_objective_value\n",
    "                max_coms = merged_coms\n",
    "    return max_coms\n",
    "\n",
    "\n",
    "def louvain_algorithm(net, obj_function=modularity, max_iter=100):\n",
    "    \"\"\"\n",
    "    Apply the Louvain algorithm to find communities in a multiplex network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze.\n",
    "    obj_function : function, optional\n",
    "        The objective function to optimize (default is modularity).\n",
    "    max_iter : int, optional\n",
    "        The maximum number of iterations to run the algorithm (default is 100).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the communities and their properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    communities = init_multiplex_communities_louvain(net, obj_function)\n",
    "    states = [communities]\n",
    "    for _ in range(max_iter):\n",
    "        new_communities = louvain_step(net, communities)\n",
    "        if new_communities['objective_function_value'] <= communities['objective_function_value']:\n",
    "            break\n",
    "        states.append(new_communities)\n",
    "        communities = new_communities\n",
    "    \n",
    "    return states     \n",
    "\n",
    "def init_multiplex_communities_louvain(net, obj_function=modularity):\n",
    "    # all representation of a node is within the same community\n",
    "    coms = list(net.slices[0])\n",
    "\n",
    "    communities = {\n",
    "        'com2nodes' : {com : [nl for nl in net.iter_node_layers() if nl[0] == com] for com in coms},\n",
    "        'node2com' : {},\n",
    "        'com_inner_weight' : {com : 0 for com in coms},\n",
    "        'com_total_weight' : {com : 0 for com in coms},\n",
    "        'graph_size' : len(net.edges),\n",
    "        'objective_function' : obj_function,\n",
    "        'objective_function_name' : obj_function.__name__,\n",
    "        'objective_function_value' : 0,\n",
    "    }\n",
    "\n",
    "\n",
    "    for com, nodes in communities['com2nodes'].items():\n",
    "        inner_w = 0\n",
    "        total_w = 0\n",
    "        for node1 in nodes:\n",
    "            for node2 in nodes:\n",
    "                inner_w += net[node1][node2]\n",
    "            for neighbor in net._iter_neighbors(node1):\n",
    "                total_w += net[node1][neighbor]\n",
    "        communities['com_inner_weight'][com] = inner_w\n",
    "        communities['com_total_weight'][com] = total_w\n",
    "    \n",
    "    communities['node2com'] = {node: com for com, nodes in communities['com2nodes'].items() for node in nodes}\n",
    "    communities['objective_function_value'] = obj_function(communities)\n",
    "    return communities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### new net generation functions #####\n",
    "\n",
    "def communities_are_neighbors(net, com1, com2, communities):\n",
    "    \"\"\"\n",
    "    Check if two communities are neighbors in the network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze.\n",
    "    com1 : str\n",
    "        The first community.\n",
    "    com2 : str\n",
    "        The second community.\n",
    "    communities : dict\n",
    "        A dictionary containing the communities of the network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the communities are neighbors, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    for node1 in communities['com2nodes'][com1]:\n",
    "        for node2 in net._iter_neighbors(node1):\n",
    "            if node2 in communities['com2nodes'][com2]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_highest_degree_node(net, communities, com):\n",
    "    \"\"\"\n",
    "    Get the node with the highest degree in a given community.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze.\n",
    "    communities : dict\n",
    "        A dictionary containing the communities of the network.\n",
    "    com : str\n",
    "        The community to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The node with the highest degree in the community.\n",
    "    \"\"\"\n",
    "\n",
    "    nodes_no_layers_degree = { n_l[0] : 0 for n_l in communities['com2nodes'][com]}\n",
    "\n",
    "    for node1 in communities['com2nodes'][com]:\n",
    "        nodes_no_layers_degree[node1[0]] += net._get_degree(node1)\n",
    "    # get the node with the highest degree\n",
    "    highest_degree_node = max(nodes_no_layers_degree, key=nodes_no_layers_degree.get)\n",
    "    return highest_degree_node\n",
    "        \n",
    "    \n",
    "def generate_louvain_communities_net(net,communities):\n",
    "    \"\"\"\n",
    "    Generate a new network from the communities of a multilayer network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze.\n",
    "    communities : dict\n",
    "        A dictionary containing the communities of the network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MultilayerNetwork\n",
    "        A new multilayer network with the communities as nodes and edges between them.\n",
    "    \"\"\"\n",
    "    \n",
    "    com_net = MultilayerNetwork(aspects=1, directed=False)\n",
    "    \n",
    "    com_2_name = {com:\"\" for com in communities['com2nodes'].keys()}\n",
    "    for com, nodes in communities['com2nodes'].items():\n",
    "        com_2_name[com] = get_highest_degree_node(net, communities, com)\n",
    "\n",
    "\n",
    "    for com in communities['com2nodes'].keys():\n",
    "        com_net.add_node(com_2_name[com])\n",
    "\n",
    "    neighbors = defaultdict(set)\n",
    "    for com in communities['com2nodes'].keys():\n",
    "        for com2 in communities['com2nodes'].keys():\n",
    "            if com != com2 and communities_are_neighbors(net, com, com2, communities):\n",
    "                com_name = com_2_name[com]\n",
    "                com2_name = com_2_name[com2]\n",
    "                neighbors[com_name].add(com2_name)\n",
    "            if com2 != com and communities_are_neighbors(net, com2, com, communities):\n",
    "                com_name = com_2_name[com]\n",
    "                com2_name = com_2_name[com2]\n",
    "                neighbors[com2_name].add(com_name)\n",
    "\n",
    "    for com, neighbors_set in neighbors.items():\n",
    "        for neighbor in neighbors_set:\n",
    "            com_net[(com,1)][(neighbor,1)] = 1  # or any weight you want, here we use 1\n",
    "\n",
    "    com_sizes = {com_2_name[com]: len(nodes) for com, nodes in communities['com2nodes'].items()}\n",
    "\n",
    "    \n",
    "\n",
    "    return com_net, com_sizes, com_2_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective function value: 0.28081717451523547\n",
      "Communities: {1: [(1, 1), (1, 2), (1, 3)], 2: [(2, 1), (2, 2), (2, 3)], 3: [(3, 1), (3, 2), (3, 3)], 4: [(4, 1), (4, 2), (4, 3)], 5: [(5, 1), (5, 2), (5, 3)], 6: [(6, 1), (6, 2), (6, 3)]}\n",
      "Objective function value: 0.3268698060941828\n",
      "Communities: {1: [(1, 1), (1, 2), (1, 3)], 2: [(2, 1), (2, 2), (2, 3), (4, 1), (4, 2), (4, 3)], 3: [(3, 1), (3, 2), (3, 3)], 5: [(5, 1), (5, 2), (5, 3)], 6: [(6, 1), (6, 2), (6, 3)]}\n",
      "Objective function value: 0.36426592797783935\n",
      "Communities: {1: [(1, 1), (1, 2), (1, 3)], 2: [(2, 1), (2, 2), (2, 3), (4, 1), (4, 2), (4, 3)], 3: [(3, 1), (3, 2), (3, 3)], 5: [(5, 1), (5, 2), (5, 3), (6, 1), (6, 2), (6, 3)]}\n"
     ]
    }
   ],
   "source": [
    "coms_states = louvain_algorithm(multinet_coms, obj_function=modularity, max_iter=100)\n",
    "for coms in coms_states:\n",
    "    print(\"Objective function value:\", coms['objective_function_value'])\n",
    "    print(\"Communities:\", coms['com2nodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "def init_multiplex_communities_leiden(net, obj_function=modularity):\n",
    "    # all representation of a node is within the same community\n",
    "    coms = list(net.slices[0])\n",
    "\n",
    "    communities = {\n",
    "        'com2nodes' : {com : [nl for nl in net.iter_node_layers() if nl[0] == com] for com in coms},\n",
    "        'node2com' : {},\n",
    "        'com_inner_weight' : {com : 0 for com in coms},\n",
    "        'com_total_weight' : {com : 0 for com in coms},\n",
    "        'graph_size' : len(net.edges),\n",
    "        'neigh_coms' : {com : set() for com in coms},\n",
    "        'objective_function' : obj_function,\n",
    "        'objective_function_name' : obj_function.__name__,\n",
    "        'objective_function_value' : 0,\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    for com, nodes in communities['com2nodes'].items():\n",
    "        inner_w = 0\n",
    "        total_w = 0\n",
    "        for node1 in nodes:\n",
    "            for node2 in nodes:\n",
    "                inner_w += net[node1][node2]\n",
    "            for neighbor in net._iter_neighbors(node1):\n",
    "                total_w += net[node1][neighbor]\n",
    "            communities['neigh_coms'][com].update([x[0] for x in net._iter_neighbors(node1) if x[0] != node1[0]])\n",
    "\n",
    "        communities['com_inner_weight'][com] = inner_w\n",
    "        communities['com_total_weight'][com] = total_w\n",
    "    \n",
    "    communities['node2com'] = {node: com for com, nodes in communities['com2nodes'].items() for node in nodes}\n",
    "    communities['objective_function_value'] = obj_function(communities)\n",
    "    return communities\n",
    "    \n",
    "def leiden_FastNodeMove(net,communities, gamma):\n",
    "    q = queue.Queue()\n",
    "    \n",
    "    coms = list(communities['com2nodes'].keys())\n",
    "    old_communities = copy.deepcopy(communities)\n",
    "\n",
    "    random.shuffle(coms)\n",
    "    for com in coms:\n",
    "        q.put(com)\n",
    "    # print(q.qsize())\n",
    "    elems_in_q = coms\n",
    "\n",
    "    merge_history = {com: [com] for com in coms}\n",
    "    \n",
    "    while not q.empty():\n",
    "        com1 = q.get()\n",
    "        elems_in_q.remove(com1)\n",
    "        # print(\"step: \", com1)\n",
    "        if com1 not in communities['com2nodes']:            \n",
    "            continue\n",
    "        max_cpm = 0\n",
    "        max_com = com1\n",
    "        for com2 in communities['neigh_coms'][com1]:\n",
    "            weights_inter_coms = weights_inter_com1_com2(net,communities,com1,com2)\n",
    "            new_cpm=compute_inc_CPM(communities,com1,com2,weights_inter_coms,gamma)\n",
    "            # print(\"  com1: \",com1,\" com2: \",com2,\" new_cpm: \",new_cpm)\n",
    "            if new_cpm > max_cpm:\n",
    "                max_com = com2\n",
    "                max_cpm = new_cpm\n",
    "        if max_cpm > 0: \n",
    "            # print(\"  merging with\",max_com)\n",
    "            # print(\"  max_cpm \",max_cpm)\n",
    "            weights_inter_coms = weights_inter_com1_com2(net,communities,com1,max_com)\n",
    "            old_neigh_com1 = communities['neigh_coms'][com1]\n",
    "            communities = merge_communities(net,communities,com1,max_com,weights_inter_coms)\n",
    "            \n",
    "            #append the merge history of max_com to com1\n",
    "            merge_history[com1] = merge_history[com1] + merge_history[max_com]\n",
    "            merge_history.pop(max_com, None)  # remove max_com from history\n",
    "\n",
    "            \n",
    "\n",
    "            for com in old_neigh_com1:\n",
    "                if com not in elems_in_q:\n",
    "                    q.put(com)   \n",
    "                    elems_in_q.append(com)\n",
    "\n",
    "    return old_communities, communities, merge_history\n",
    "\n",
    "def weights_inter_com1_com2(net,coms,com1,com2):\n",
    "    weight_inter_coms = 0\n",
    "    for node1 in coms['com2nodes'][com1]:\n",
    "        for neighbor in net._iter_neighbors(node1):\n",
    "            if coms['node2com'][neighbor] == com2:\n",
    "                weight_inter_coms += net[node1][neighbor]\n",
    "    return weight_inter_coms\n",
    "\n",
    "def weights_inter_subcom(net, subcoms, subcoms_merged):\n",
    "    edge_weights = { subcom: 0 for subcom in subcoms_merged }\n",
    "    for i in range(len(subcoms_merged)):\n",
    "        subcom = subcoms_merged[i]\n",
    "        for j in range(i + 1, len(subcoms_merged)):\n",
    "            other_subcom = subcoms_merged[j]\n",
    "            if subcom == other_subcom:\n",
    "                continue\n",
    "            weight = weights_inter_com1_com2(net, subcoms, subcom, other_subcom)\n",
    "            edge_weights[subcom] += weight\n",
    "            edge_weights[other_subcom] += weight\n",
    "    return edge_weights\n",
    "                        \n",
    "def compute_inc_CPM(communities,com1,com2,weights_inter_com1_com2,gamma):\n",
    "    com1_inner = communities['com_inner_weight'][com1]\n",
    "    com1_size = len(communities['com2nodes'][com1])\n",
    "    com2_inner = communities['com_inner_weight'][com2]\n",
    "    com2_size = len(communities['com2nodes'][com2])\n",
    "    dec = (com1_inner - gamma*(com1_size*(com1_size-1)/2)) + (com2_inner - gamma*(com2_size*(com2_size-1)/2))\n",
    "    inc = (com1_inner + com2_inner + weights_inter_com1_com2) - gamma*(com1_size + com2_size)*(com1_size + com2_size - 1)/2\n",
    "    return inc - dec\n",
    "\n",
    "def merge_communities(net,communities,com1,com2,weigth_inter_coms):\n",
    "    communities = communities.copy()\n",
    "    # merge both communities into the new one\n",
    "    communities['com2nodes'][com1] = communities['com2nodes'][com1] + communities['com2nodes'][com2]\n",
    "    # change all nodes to the new community\n",
    "    for node in communities['com2nodes'][com2]:\n",
    "        communities['node2com'][node] = com1\n",
    "    \n",
    "\n",
    "    # the new internal weight is the sum of both plus twice the sum between the communities\n",
    "    communities['com_inner_weight'][com1] += communities['com_inner_weight'][com2] + 2*weigth_inter_coms\n",
    "\n",
    "    # new neighboors\n",
    "    communities['neigh_coms'][com1] = communities['neigh_coms'][com1].union(communities['neigh_coms'][com2])\n",
    "    communities['neigh_coms'][com1].discard(com1)  # remove self-loop if exists\n",
    "    communities['neigh_coms'][com1].discard(com2)  # remove self-loop if exists\n",
    "\n",
    "    communities['com_total_weight'][com1] = communities['com_total_weight'][com1] + communities['com_total_weight'][com2] + weigth_inter_coms\n",
    "\n",
    "    #remove the old community\n",
    "    communities['com2nodes'].pop(com2)\n",
    "    communities['com_inner_weight'].pop(com2)\n",
    "    communities['com_total_weight'].pop(com2)\n",
    "    communities['neigh_coms'].pop(com2)\n",
    "\n",
    "    # update the neigh_coms of the neighbors, change com2 for com1 in the neigh_coms for other communities\n",
    "    for k,v in communities['neigh_coms'].items():\n",
    "        if com2 in v:\n",
    "            v.remove(com2)\n",
    "            v.add(com1)\n",
    "    # if com1 in communities['neigh_coms'][com1]:\n",
    "    #     communities['neigh_coms'][com1].remove(com1)\n",
    "\n",
    "    communities['objective_function_value'] = communities['objective_function'](communities)\n",
    "\n",
    "    return communities\n",
    "        \n",
    "def check_interconected_subcoms(net, parent_coms, subcoms, parent_com, subcoms_in_parent, gamma):\n",
    "        # print(parent_com, subcoms_in_parent)\n",
    "        weights_inter_subcoms = weights_inter_subcom(net, subcoms, subcoms_in_parent)\n",
    "        R = []\n",
    "        for subcom1 in subcoms_in_parent:\n",
    "\n",
    "            e_v_S = weights_inter_subcoms[subcom1]\n",
    "            # norm_v = deg of all nodes in subcom1\n",
    "            norm_v = len(subcoms['com2nodes'][subcom1])\n",
    "            norm_S = len(parent_coms['com2nodes'][parent_com])\n",
    "            # edges(subcom1, com - subcom1) >= gamma * ||subcom1|| * (||com|| - ||subcom1||) \n",
    "            if e_v_S >= gamma * norm_v * (norm_S - norm_v):\n",
    "                R.append(subcom1)\n",
    "                continue\n",
    "        random.shuffle(R)\n",
    "        return R\n",
    "\n",
    "def refine_partition(net, parent_coms, subcoms, merge_history, gamma):\n",
    "    for com_merged, subcoms_merged in merge_history.items():\n",
    "        # print(\"Refining partition for com_merged:\", com_merged, \"with subcoms_merged:\", subcoms_merged)\n",
    "        R = check_interconected_subcoms(net, parent_coms, subcoms, com_merged, subcoms_merged, gamma)\n",
    "        singleton = R.copy()\n",
    "        # print(\"R: \", R)\n",
    "        if len(R) < 2:\n",
    "            # print(\"Skipping refinement for com_merged:\", com_merged, \"as R has less than 2 elements.\")\n",
    "            continue\n",
    "        for subcom1 in R:\n",
    "            # check if subcom1 is a singleton using singleton list\n",
    "            if subcom1 not in singleton:\n",
    "                continue\n",
    "\n",
    "\n",
    "            T = check_interconected_subcoms(net, parent_coms, subcoms, com_merged, subcoms_merged, gamma)\n",
    "            # print(\"     T: \", T)\n",
    "            if not T:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # Compute ΔH for each target subcommunity\n",
    "            inc_CPMs = []\n",
    "            for subcom2 in T:\n",
    "                if subcom1 == subcom2:\n",
    "                    continue\n",
    "                weights_com1_com2 = weights_inter_com1_com2(net, subcoms, subcom1, subcom2)\n",
    "                inc_CPM = compute_inc_CPM(subcoms, subcom1, subcom2,weights_com1_com2, gamma)\n",
    "                inc_CPMs.append((subcom2, inc_CPM))\n",
    "\n",
    "                        # Softmax-based random selection\n",
    "            probs = []\n",
    "            for (subcom2, inc_CPM) in inc_CPMs:\n",
    "                # print(\"         subcom1:\", subcom1, \"subcom2:\", subcom2, \"inc_CPM:\", inc_CPM)\n",
    "                if inc_CPM > 0:\n",
    "                    # print(math.exp(0.5 * inc_CPM))\n",
    "                    probs.append(math.exp(0.5 * inc_CPM))\n",
    "                else:\n",
    "                    probs.append(0.0)\n",
    "\n",
    "            if sum(probs) == 0:\n",
    "                continue\n",
    "\n",
    "            chosen = random.choices([x[0] for x in inc_CPMs], weights=probs, k=1)[0]\n",
    "\n",
    "            if subcom1 in singleton:\n",
    "                singleton.remove(subcom1)\n",
    "            if chosen in singleton:\n",
    "                singleton.remove(chosen)\n",
    "            # print(\"         singleton: \", singleton)\n",
    "\n",
    "            # print('         merging subcom1:', subcom1, 'with subcom2:', chosen, 'with inc_CPM:', inc_CPMs)\n",
    "            subcoms = merge_communities(net, subcoms, chosen, subcom1, \n",
    "                                             weights_inter_com1_com2(net, subcoms, chosen, subcom1))\n",
    "\n",
    "            subcoms_merged.remove(subcom1)\n",
    "\n",
    "\n",
    "    return subcoms\n",
    "\n",
    "\n",
    "def leiden_algorithm(net, gamma=1.0, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Perform the Leiden algorithm for community detection in a multiplex network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze.\n",
    "    gamma : float, optional\n",
    "        Resolution parameter for the Leiden algorithm. Default is 1.0.\n",
    "    max_iterations : int, optional\n",
    "        Maximum number of iterations to run the algorithm. Default is 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the final communities and their properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    communities = init_multiplex_communities_leiden(net)\n",
    "    states = [communities]\n",
    "    for _ in range(max_iterations):\n",
    "        old_communities, communities, merge_history = leiden_FastNodeMove(net, communities, gamma)\n",
    "        communities = refine_partition(net, communities,old_communities, merge_history, gamma)\n",
    "\n",
    "        # check if the com2nodes of old_communities and communities are the same\n",
    "        if old_communities['com2nodes'] == communities['com2nodes']:\n",
    "            print(\"Convergence reached.\")\n",
    "            break\n",
    "        else:\n",
    "            states.append(communities)\n",
    "\n",
    "\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_leiden_communities_net(net, communities):\n",
    "    \"\"\"\n",
    "    Generate a new network from the communities of a multiplex network using the Leiden algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    net : MultilayerNetwork\n",
    "        The multilayer network to analyze.\n",
    "    communities : dict\n",
    "        A dictionary containing the communities of the network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MultilayerNetwork\n",
    "        A new multilayer network with the communities as nodes and edges between them.\n",
    "    \"\"\"\n",
    "    \n",
    "    com_net = MultilayerNetwork(aspects=1, directed=False)\n",
    "\n",
    "\n",
    "    \n",
    "    com_2_name = {com:\"\" for com in communities['com2nodes'].keys()}\n",
    "    for com, nodes in communities['com2nodes'].items():\n",
    "        com_2_name[com] = get_highest_degree_node(net, communities, com)\n",
    "\n",
    "\n",
    "    for com in communities['com2nodes'].keys():\n",
    "        com_net.add_node(com_2_name[com])\n",
    "\n",
    "    neighbors = defaultdict(set)\n",
    "    for com in communities['com2nodes'].keys():\n",
    "        for com2 in communities['neigh_coms'][com]:\n",
    "            if com != com2:\n",
    "                com_name = com_2_name[com]\n",
    "                com2_name = com_2_name[com2]\n",
    "                neighbors[com_name].add(com2_name)\n",
    "    for com, neighbors_set in neighbors.items():\n",
    "        for neighbor in neighbors_set:\n",
    "            com_net[(com,1)][(neighbor,1)] = 1  # or any weight you want, here we use 1\n",
    "\n",
    "    com_sizes = {com_2_name[com]: len(nodes) for com, nodes in communities['com2nodes'].items()}\n",
    "\n",
    "    return com_net, com_sizes, com_2_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached.\n",
      "Objective function value: 0.28081717451523547\n",
      "Communities: {1: [(1, 1), (1, 2), (1, 3)], 2: [(2, 1), (2, 2), (2, 3)], 3: [(3, 1), (3, 2), (3, 3)], 4: [(4, 1), (4, 2), (4, 3)], 5: [(5, 1), (5, 2), (5, 3)], 6: [(6, 1), (6, 2), (6, 3)]}\n"
     ]
    }
   ],
   "source": [
    "coms_states = leiden_algorithm(multinet_coms, gamma=1.0, max_iterations=100)\n",
    "for coms in coms_states:\n",
    "    print(\"Objective function value:\", coms['objective_function_value'])\n",
    "    print(\"Communities:\", coms['com2nodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 ---Difusion---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(nl1_state, nl2_state,gamma_nl1,gamma_nl2,beta_nl1_nl2,beta_nl2_nl1):\n",
    "    #begin cases\n",
    "    match (nl1_state,nl2_state):\n",
    "\n",
    "        case (1,1):\n",
    "            return (np.random.choice([1,2], p=[1-gamma_nl1,gamma_nl1]),\n",
    "                    np.random.choice([1,2], p=[1-gamma_nl2,gamma_nl2])) \n",
    "        case (0,1):\n",
    "            return (np.random.choice([0,1], p=[1-beta_nl2_nl1,beta_nl2_nl1]),\n",
    "                    np.random.choice([1,2], p=[1-gamma_nl2,gamma_nl2]))\n",
    "        case (1,0):\n",
    "            return (np.random.choice([1,2], p=[1-gamma_nl1,gamma_nl1]),\n",
    "                    np.random.choice([0,1], p=[1-beta_nl1_nl2,beta_nl1_nl2]))\n",
    "        \n",
    "        case (1,2):\n",
    "            return (np.random.choice([1,2], p=[1-gamma_nl1,gamma_nl1]),\n",
    "                    2)\n",
    "        case (2,1):\n",
    "            return (2,\n",
    "                    np.random.choice([1,2], p=[1-gamma_nl2,gamma_nl2]))\n",
    "        case default:\n",
    "            return (nl1_state,nl2_state)\n",
    "\n",
    "def SIR_net_diffusion(self, interlayers_beta , intra_gamma, iterations, initial_state=None):\n",
    "    \"\"\"Given a multilayer network and a dictionary with\n",
    "        the labels of the nodes in the form of a tuple (node, layer),\n",
    "        this function returns the vector of the labels of the nodes\n",
    "        after the number of iterations specified, using the SIR model.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        net : MultilayerNetwork\n",
    "            The multilayer network to analyze\n",
    "        intra_beta : float\n",
    "            The beta parameter of the SIR model for the intralayer edges\n",
    "        inter_beta : float\n",
    "            The beta parameter of the SIR model for the interlayer edges\n",
    "        gamma : float\n",
    "            The gamma parameter of the SIR model for each layer\n",
    "        iterations : int\n",
    "            The number of iterations to perform the diffusion\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "    \"\"\"\n",
    "    \n",
    "    if not initial_state:\n",
    "        initial_state = {edge_nodes: np.random.choice([0,1],p=[0.8,0.2]) for edge in self.edges for edge_nodes in self._link_to_nodes(edge[:-1])}\n",
    "\n",
    "    state_list = [initial_state.copy()]\n",
    "    state = initial_state\n",
    "\n",
    "    for i in range(iterations):\n",
    "        state_changed = False  # Track if any node state changes\n",
    "        new_state = state.copy()\n",
    "        for edge in self.edges:\n",
    "            ((nl1_0,nl1_1, weight),nl2) = self._link_to_nodes(edge)\n",
    "            nl1 = (nl1_0, nl1_1)\n",
    "            nl1_state = state[nl1]\n",
    "            nl2_state = state[nl2]\n",
    "\n",
    "            gamma_nl1 = intra_gamma[tuple(nl1[1:])]\n",
    "            gamma_nl2 = intra_gamma[tuple(nl2[1:])]\n",
    "            beta_nl1_nl2 = interlayers_beta[(nl1[1:], nl2[1:])]\n",
    "            beta_nl2_nl1 = interlayers_beta[(nl2[1:], nl1[1:])]\n",
    "\n",
    "\n",
    "            # Scale beta by weight\n",
    "            beta_nl1_nl2 = min(beta_nl1_nl2 * weight, 1.0)\n",
    "            beta_nl2_nl1 = min(beta_nl2_nl1 * weight, 1.0)\n",
    "\n",
    "            updated_nl1, updated_nl2  = update_state(\n",
    "                nl1_state, nl2_state,\n",
    "                gamma_nl1, gamma_nl2,\n",
    "                beta_nl1_nl2, beta_nl2_nl1\n",
    "            )\n",
    "\n",
    "            if new_state[nl1] != updated_nl1:\n",
    "                new_state[nl1] = updated_nl1\n",
    "                state_changed = True\n",
    "\n",
    "            if new_state[nl2] != updated_nl2:\n",
    "                new_state[nl2] = updated_nl2\n",
    "                state_changed = True\n",
    "\n",
    "        # Stop if nothing changed\n",
    "        if not state_changed:\n",
    "            break\n",
    "\n",
    "\n",
    "        state = new_state\n",
    "        state_list.append(state.copy())\n",
    "        #if all states are 0 or 2, then the epidemic is over\n",
    "        if all(s in (0, 2) for s in state.values()):\n",
    "            break\n",
    "            \n",
    "    return state_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply to Madrid Transport Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ---read from csv---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nodes_to_link(node1, node2):\n",
    "    \"\"\"Return a link when tuple of nodes is given in the graph representing\n",
    "    the multislice structure. I.e. when given (i,s_1,...,s_d),(j,r_1,...,r_d)\n",
    "    (i,j,s_1,r_1, ... ,s_d,r_d) is returned.\n",
    "    \"\"\"\n",
    "    assert len(node1) == len(node2)\n",
    "    l = []\n",
    "    for i, n1 in enumerate(node1):\n",
    "        l.append(n1)\n",
    "        l.append(node2[i])\n",
    "    return tuple(l)\n",
    "\n",
    "\n",
    "def read_multiplex_node_layer_edges_from_csv(file_path, weighted=False, directed=False):\n",
    "    \"\"\"\n",
    "    Reads multilayer edges from a CSV in node,layer,node,layer format and returns a pymnet network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV file.\n",
    "    weighted : bool\n",
    "        If True, assumes the last column is edge weight.\n",
    "    directed : bool\n",
    "        If True, creates a directed network.\n",
    "    couplings : str\n",
    "        Coupling type for pymnet network (only applies to Multiplex).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MultilayerNetwork\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Handle weights if present\n",
    "    if weighted:\n",
    "        weights = df.iloc[:, -1]\n",
    "        edge_data = df.iloc[:, :-1]\n",
    "    else:\n",
    "        weights = pd.Series([1] * len(df))\n",
    "        edge_data = df\n",
    "\n",
    "    if edge_data.shape[1] != 4:\n",
    "        raise ValueError(\"Expected columns: node1, layer1, node2, layer2 (plus optional weight).\")\n",
    "\n",
    "    # Initialize general multilayer network\n",
    "    net = MultilayerNetwork(directed=directed, aspects=1)\n",
    "\n",
    "    # Add edges\n",
    "    for (_, row), weight in zip(edge_data.iterrows(), weights):\n",
    "        u = (row[0], row[1])  # (node1, layer1)\n",
    "        v = (row[2], row[3])  # (node2, layer2)\n",
    "        net[u][v] = weight\n",
    "\n",
    "    return net\n",
    "\n",
    "def read_multiplex_edges_from_csv(file_path, weighted=False, directed=False):\n",
    "    \"\"\"\n",
    "    Reads multilayer edges from a CSV and returns a pymnet network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV file.\n",
    "    weighted : bool\n",
    "        If True, assumes the last column is edge weight.\n",
    "    directed : bool\n",
    "        If True, creates a directed network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MultiplexNetwork\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Handle weights if present\n",
    "    if weighted:\n",
    "        weights = df.iloc[:, -1]\n",
    "        edge_data = df.iloc[:, :-1]\n",
    "    else:\n",
    "        weights = pd.Series([1] * len(df))\n",
    "        edge_data = df.iloc[:, :]\n",
    "    # Detect number of layers from column count\n",
    "    n_cols = edge_data.shape[1]\n",
    "    if n_cols % 2 != 0:\n",
    "        raise ValueError(\"Number of columns must be even (pairs of node-layer).\")\n",
    "\n",
    "    # Initialize network\n",
    "    net = MultilayerNetwork(directed=directed, aspects=1)\n",
    "    #min wight > 0\n",
    "    min_weight = weights[weights > 0].min() if weighted else 1\n",
    "\n",
    "\n",
    "    # Add edges\n",
    "    for (_, row), weight in zip(edge_data.iterrows(), weights):\n",
    "        # net[tuple(row)] =1 \n",
    "\n",
    "        net[tuple(row)] = min_weight/weight if weight > 0 else 0\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_path = '../Data/Final_data/madrid_transport_nodes.csv'\n",
    "shp_nodes_path = '../Data/Final_data/final_nodes.csv'\n",
    "edges_path = '../Data/Final_data/madrid_transport_edges.csv'\n",
    "edges_no_weight_path = '../Data/Final_data/madrid_transport_edges_no_weight.csv'\n",
    "madrid_transport_nodes_shp = pd.read_csv(shp_nodes_path)\n",
    "\n",
    "node_coords = {\n",
    "    row['stop']: (row['stop_lon'], row['stop_lat'])  # X = lon, Y = lat\n",
    "    for _, row in madrid_transport_nodes_shp.iterrows()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnet_weighted = read_multiplex_edges_from_csv(edges_path, weighted=True)\n",
    "mnet_no_weighted = read_multiplex_edges_from_csv(edges_no_weight_path, weighted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot basic map\n",
    "\n",
    "important_nodes = [\n",
    "    'AVENIDA DE AMERICA',\n",
    "    'USERA',\n",
    "    'BARRIO DEL PILAR',\n",
    "    'PUERTA DEL SOL',\n",
    "    'LA MORALEJA',\n",
    "    'PUERTA DE TOLEDO',\n",
    "    'PUERTA DEL SUR']\n",
    "\n",
    "\n",
    "# Custom labels for selected stops\n",
    "custom_node_labels = {  node:node for node in important_nodes }\n",
    "# Assign a unique color to each node (using a color palette or named colors)\n",
    "custom_node_colors = { node: 'red' for node in important_nodes }\n",
    "\n",
    "\n",
    "nodeAlphaDict = {\n",
    "    (node, layer): 0.6 if node in custom_node_labels else 0.4\n",
    "    for (node, layer) in mnet_no_weighted.iter_node_layers()\n",
    "}\n",
    "nodeSizeDict = {\n",
    "    (node, layer): 0.01 if node in custom_node_labels else 0.005\n",
    "    for (node, layer) in mnet_no_weighted.iter_node_layers()\n",
    "}\n",
    "\n",
    "# nodeSizeRuleDict={\"rule\": \"scaled\", \"scalecoeff\": 0.15},\n",
    "# Build nodeLabelDict and nodeColorDict for all matching node-layer pairs\n",
    "nodeLabelDict = {\n",
    "    (node, layer): custom_node_labels[node]\n",
    "    for (node, layer) in mnet_no_weighted.iter_node_layers()\n",
    "    if node in custom_node_labels\n",
    "}\n",
    "\n",
    "nodeColorDict = {\n",
    "    (node, layer): custom_node_colors[node]\n",
    "    for (node, layer) in mnet_no_weighted.iter_node_layers()\n",
    "    if node in custom_node_colors\n",
    "}\n",
    "\n",
    "# Draw the multilayer network\n",
    "fig = plt.figure(figsize=(36, 24))\n",
    "ax_n = fig.add_subplot(111, projection='3d')\n",
    "draw(\n",
    "    mnet_no_weighted,\n",
    "    nodeLabelDict=nodeLabelDict,       # Show only selected labels\n",
    "    nodeLabelRule={},                  # No automatic labels\n",
    "    nodeCoords=node_coords,            # Use shapefile-coordinates\n",
    "    defaultNodeLabel=\"\",               # Hide others\n",
    "    defaultLayerLabelLoc=(-0.1, 0.1),  # Position layer names\n",
    "    defaultEdgeAlpha=0.4,              # Edge transparency\n",
    "    defaultNodeLabelAlpha=1,\n",
    "    defaultNodeLabelColor='red',\n",
    "    \n",
    "    nodeColorDict=nodeColorDict,       # Custom node colors\n",
    "    nodeSizeDict=nodeSizeDict,         # Custom node sizes\n",
    "    layergap=1,                        # Layer vertical separation\n",
    "    ax=ax_n\n",
    ")\n",
    "\n",
    "fig.savefig('madrid_transport_network.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached.\n",
      "1\n",
      "communities found by leiden algorithm: 14\n"
     ]
    }
   ],
   "source": [
    "# --- First: Plot leiden_net with community size scaling ---\n",
    "# Scale node sizes using leiden_com_sizes (adjust scale factor as needed)\n",
    "GAMMA = 0.0005\n",
    "\n",
    "leiden_states = leiden_algorithm(mnet_weighted, gamma=GAMMA, max_iterations=100)\n",
    "print(len(leiden_states))\n",
    "print('communities found by leiden algorithm:', len(leiden_states[-1]['com2nodes']))\n",
    "leiden_com_net, leiden_com_sizes, leiden_com_2_name = generate_leiden_communities_net(mnet_weighted, leiden_states[-1])\n",
    "\n",
    "\n",
    "nodeSizeDict_leiden = {\n",
    "    (node, layer): leiden_com_sizes[node] / 1300 # Tune 0.5 as needed\n",
    "    for (node, layer) in leiden_com_net.iter_node_layers()\n",
    "    if node in leiden_com_sizes\n",
    "}\n",
    "nodeSizeDict_global = {\n",
    "    (node, layer): 0.01 for (node, layer) in mnet_weighted.iter_node_layers()\n",
    "}\n",
    "\n",
    "\n",
    "node_coords = {\n",
    "    row['stop']: (row['stop_lon'], row['stop_lat'])  # X = lon, Y = lat\n",
    "    for _, row in madrid_transport_nodes_shp.iterrows()\n",
    "}\n",
    "colors = plt.get_cmap('tab20', len(leiden_com_net.slices[0]))\n",
    "# map each community to a color\n",
    "nodeColorDict_leiden = {\n",
    "    node: colors(i) if node in leiden_com_sizes else 'black'\n",
    "    for i, node in enumerate(leiden_com_net.slices[0])\n",
    "    if node in leiden_com_sizes\n",
    "}\n",
    "\n",
    "# for each node in the community, assign the color of the community\n",
    "nodeColorDict_leiden = {\n",
    "    (node, layer): nodeColorDict_leiden[node]\n",
    "    for (node, layer) in leiden_com_net.iter_node_layers()\n",
    "    if node in nodeColorDict_leiden\n",
    "}\n",
    "# change GETAFE CENTRAL color to COLOMBIA color and vice versa\n",
    "nodeColorDict_global = {\n",
    "    node:  nodeColorDict_leiden[( leiden_com_2_name[com],1)]\n",
    "    for com, nodes in leiden_states[-1]['com2nodes'].items()\n",
    "    for node in nodes\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#### PLOTS ####\n",
    "fig1 = plt.figure(figsize=(36, 24))\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "\n",
    "draw(\n",
    "    leiden_com_net,\n",
    "    nodeCoords=node_coords,\n",
    "    nodeColorDict=nodeColorDict_leiden,  # Use custom colors for communities\n",
    "    defaultNodeLabel=\"\",               # Hide labels\n",
    "    defaultEdgeAlpha=0.4,\n",
    "    nodeSizeDict=nodeSizeDict_leiden, # Scaled by community size\n",
    "    layergap=1,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "custom_node_labels = {  node:node for node in leiden_com_net.slices[0] }\n",
    "\n",
    "\n",
    "fig1.savefig(f'leiden_communities_scaled_gamma_{GAMMA}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# --- Second: Plot original net with community nodes in red ---\n",
    "# Identify nodes in leiden_net\n",
    "leiden_nodes = set(node for node, _ in leiden_com_net.iter_node_layers())\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize=(36, 24))\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "draw(\n",
    "    mnet_weighted,\n",
    "    nodeCoords=node_coords,\n",
    "    nodeSizeDict=nodeSizeDict_global,  # Use global node sizes\n",
    "    nodeColorDict=nodeColorDict_global,\n",
    "    nodeLabelRule={},                  # No automatic labels\n",
    "    defaultNodeLabel=\"\",\n",
    "    defaultEdgeAlpha=0.3,\n",
    "    layergap=1,\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "fig2.savefig(f'original_network_highlighted_leiden_nodes_gamma_{GAMMA}.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "communities found by louvain algorithm: 142\n"
     ]
    }
   ],
   "source": [
    "# --- First: Plot louvain_net with community size scaling ---\n",
    "# Scale node sizes using louvain_com_sizes (adjust scale factor as needed)\n",
    "\n",
    "louvain_states = louvain_algorithm(mnet_no_weighted)\n",
    "print(len(louvain_states))\n",
    "print('communities found by louvain algorithm:', len(louvain_states[-1]['com2nodes']))\n",
    "louvain_com_net, louvain_com_sizes, louvain_com_2_name = generate_louvain_communities_net(mnet_no_weighted, louvain_states[-1])\n",
    "\n",
    "\n",
    "nodeSizeDict_louvain = {\n",
    "    (node, layer): louvain_com_sizes[node] / 1000 # Tune 0.5 as needed\n",
    "    for (node, layer) in louvain_com_net.iter_node_layers()\n",
    "    if node in louvain_com_sizes\n",
    "}\n",
    "nodeSizeDict_global = {\n",
    "    (node, layer): 0.01 for (node, layer) in mnet_no_weighted.iter_node_layers()\n",
    "}\n",
    "\n",
    "\n",
    "node_coords = {\n",
    "    row['stop']: (row['stop_lon'], row['stop_lat'])  # X = lon, Y = lat\n",
    "    for _, row in madrid_transport_nodes_shp.iterrows()\n",
    "}\n",
    "colors = plt.get_cmap('tab20', len(louvain_com_net.slices[0]))\n",
    "# map each community to a color\n",
    "nodeColorDict_louvain = {\n",
    "    node: colors(i) if node in louvain_com_sizes else 'black'\n",
    "    for i, node in enumerate(louvain_com_net.slices[0])\n",
    "    if node in louvain_com_sizes\n",
    "}\n",
    "\n",
    "# for each node in the community, assign the color of the community\n",
    "nodeColorDict_louvain = {\n",
    "    (node, layer): nodeColorDict_louvain[node]\n",
    "    for (node, layer) in louvain_com_net.iter_node_layers()\n",
    "    if node in nodeColorDict_louvain\n",
    "}\n",
    "# change GETAFE CENTRAL color to COLOMBIA color and vice versa\n",
    "nodeColorDict_global = {\n",
    "    node:  nodeColorDict_louvain[( louvain_com_2_name[com],1)]\n",
    "    for com, nodes in louvain_states[-1]['com2nodes'].items()\n",
    "    for node in nodes\n",
    "}  \n",
    "\n",
    "#show only labeles of communities with more than 6 nodes \n",
    "nodeLabelDict_louvain = {\n",
    "    (node, layer): node\n",
    "    for (node, layer) in louvain_com_net.iter_node_layers()\n",
    "    if node in louvain_com_sizes and louvain_com_sizes[node] > 9\n",
    "}\n",
    "\n",
    "\n",
    "#### PLOTS ####\n",
    "fig1 = plt.figure(figsize=(36, 24))\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "\n",
    "draw(\n",
    "    louvain_com_net,\n",
    "    nodeCoords=node_coords,\n",
    "    nodeColorDict=nodeColorDict_louvain,  # Use custom colors for communities\n",
    "    nodeLabelDict=nodeLabelDict_louvain,  # Show only selected labels\n",
    "    nodeLabelRule={},                  # No automatic labels\n",
    "    defaultNodeLabel=\"\",               # Hide labels\n",
    "    defaultEdgeAlpha=0.4,\n",
    "    nodeSizeDict=nodeSizeDict_louvain, # Scaled by community size\n",
    "    layergap=1,\n",
    "    ax=ax1\n",
    ")\n",
    "\n",
    "custom_node_labels = {  node:node for node in louvain_com_net.slices[0] }\n",
    "\n",
    "\n",
    "fig1.savefig(f'imgs/louvain_communities_scaled.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# --- Second: Plot original net with community nodes in red ---\n",
    "# Identify nodes in louvain_net\n",
    "louvain_nodes = set(node for node, _ in louvain_com_net.iter_node_layers())\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize=(36, 24))\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "draw(\n",
    "    mnet_no_weighted,\n",
    "    nodeCoords=node_coords,\n",
    "    nodeSizeDict=nodeSizeDict_global,  # Use global node sizes\n",
    "    nodeColorDict=nodeColorDict_global,\n",
    "    nodeLabelRule={},                  # No automatic labels\n",
    "    defaultNodeLabel=\"\",\n",
    "    defaultEdgeAlpha=0.3,\n",
    "    layergap=1,\n",
    "    ax=ax2\n",
    ")\n",
    "\n",
    "fig2.savefig(f'imgs/original_network_highlighted_louvain_nodes.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. ---Tensorflow functions to Madrid data---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[  0   0   0   2]\n",
      " [  0   0   2   0]\n",
      " [  0  17   0   0]\n",
      " ...\n",
      " [241 226   0   0]\n",
      " [241 241   0   2]\n",
      " [241 241   2   0]], shape=(2488, 4), dtype=int64), values=tf.Tensor([0.12346772 0.12346772 0.02445602 ... 0.32881767 0.09640628 0.09640628], shape=(2488,), dtype=float32), dense_shape=tf.Tensor([242 242   3   3], shape=(4,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "sparse, nodes, layers, inv_nodes, inv_layers = __get_sparse_tensor__(mnet_weighted, return_mappings=True)\n",
    "sparse_inter = __get_sparse_tensor_between_layers__(mnet_weighted, ['metro'], ['urban'])\n",
    "print(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABRANTES',\n",
       " 'ACACIAS',\n",
       " 'AEROPUERTO T-4',\n",
       " 'AEROPUERTO T1-T2-T3',\n",
       " 'ALAMEDA DE OSUNA',\n",
       " 'ALCORCON CENTRAL',\n",
       " 'ALFONSO XIII',\n",
       " 'ALMENDRALES',\n",
       " 'ALONSO CANO',\n",
       " 'ALONSO DE MENDOZA',\n",
       " 'ALONSO MARTINEZ',\n",
       " 'ALSACIA',\n",
       " 'ALTO DE EXTREMADURA',\n",
       " 'ALTO DEL ARENAL',\n",
       " 'ALUCHE',\n",
       " 'ALVARADO',\n",
       " 'ANTON MARTIN',\n",
       " 'ANTONIO MACHADO',\n",
       " 'ARGANDA DEL REY',\n",
       " 'ARGANZUELA-PLANETARIO',\n",
       " 'ARGÜELLES',\n",
       " 'ARROYO CULEBRO',\n",
       " 'ARROYOFRESNO',\n",
       " 'ARTILLEROS',\n",
       " 'ARTURO SORIA',\n",
       " 'ASCAO',\n",
       " 'ATOCHA',\n",
       " 'AVENIDA DE AMERICA',\n",
       " 'AVENIDA DE GUADALAJARA',\n",
       " 'AVENIDA DE LA ILUSTRACION',\n",
       " 'AVENIDA DE LA PAZ',\n",
       " 'AVIACION ESPAÑOLA',\n",
       " 'BAMBU',\n",
       " 'BANCO DE ESPAÑA',\n",
       " 'BARAJAS',\n",
       " 'BARRIO DE LA CONCEPCION',\n",
       " 'BARRIO DEL PILAR',\n",
       " 'BARRIO DEL PUERTO',\n",
       " 'BATAN',\n",
       " 'BAUNATAL',\n",
       " 'BEGOÑA',\n",
       " 'BILBAO',\n",
       " 'BUENOS AIRES',\n",
       " 'CALLAO',\n",
       " 'CAMPAMENTO',\n",
       " 'CANAL',\n",
       " 'CANILLAS',\n",
       " 'CANILLEJAS',\n",
       " 'CARABANCHEL',\n",
       " 'CARABANCHEL ALTO',\n",
       " 'CARPETANA',\n",
       " 'CARTAGENA',\n",
       " 'CASA DE CAMPO',\n",
       " 'CASA DEL RELOJ',\n",
       " 'CHAMARTIN',\n",
       " 'CHUECA',\n",
       " 'CIUDAD DE LOS ANGELES',\n",
       " 'CIUDAD LINEAL',\n",
       " 'CIUDAD UNIVERSITARIA',\n",
       " 'COLOMBIA',\n",
       " 'COLON',\n",
       " 'COLONIA JARDIN',\n",
       " 'CONCHA ESPINA',\n",
       " 'CONDE DE CASAL',\n",
       " 'CONGOSTO',\n",
       " 'CONSERVATORIO',\n",
       " 'COSLADA CENTRAL',\n",
       " 'CRUZ DEL RAYO',\n",
       " 'CUATRO CAMINOS',\n",
       " 'CUATRO VIENTOS',\n",
       " 'CUZCO',\n",
       " 'DELICIAS',\n",
       " 'DIEGO DE LEON',\n",
       " 'DUQUE DE PASTRANA',\n",
       " 'EL BERCIAL',\n",
       " 'EL CAPRICHO',\n",
       " 'EL CARMEN',\n",
       " 'EL CARRASCAL',\n",
       " 'EL CASAR',\n",
       " 'EMBAJADORES',\n",
       " 'EMPALME',\n",
       " 'ESPERANZA',\n",
       " 'ESTACION DEL ARTE',\n",
       " 'ESTADIO METROPOLITANO',\n",
       " 'ESTRECHO',\n",
       " 'ESTRELLA',\n",
       " 'EUGENIA DE MONTIJO',\n",
       " 'FERIA DE MADRID',\n",
       " 'FRANCOS RODRIGUEZ',\n",
       " 'FUENCARRAL',\n",
       " 'FUENLABRADA CENTRAL',\n",
       " 'GARCIA NOBLEJAS',\n",
       " 'GETAFE CENTRAL',\n",
       " 'GOYA',\n",
       " 'GRAN VIA',\n",
       " 'GREGORIO MARAÑON',\n",
       " 'GUZMAN EL BUENO',\n",
       " 'HENARES',\n",
       " 'HERRERA ORIA',\n",
       " 'HORTALEZA',\n",
       " 'HOSPITAL 12 DE OCTUBRE',\n",
       " 'HOSPITAL DE FUENLABRADA',\n",
       " 'HOSPITAL DE MOSTOLES',\n",
       " 'HOSPITAL DEL HENARES',\n",
       " 'HOSPITAL INFANTA SOFIA',\n",
       " 'HOSPITAL SEVERO OCHOA',\n",
       " 'IBIZA',\n",
       " 'IGLESIA',\n",
       " 'ISLAS FILIPINAS',\n",
       " 'JARAMA',\n",
       " 'JOAQUIN VILUMBRALES',\n",
       " 'JUAN DE LA CIERVA',\n",
       " 'JULIAN BESTEIRO',\n",
       " 'LA ALMUDENA',\n",
       " 'LA ELIPA',\n",
       " 'LA FORTUNA',\n",
       " 'LA GAVIA',\n",
       " 'LA GRANJA',\n",
       " 'LA LATINA',\n",
       " 'LA MORALEJA',\n",
       " 'LA PESETA',\n",
       " 'LA POVEDA',\n",
       " 'LA RAMBLA',\n",
       " 'LACOMA',\n",
       " 'LAGO',\n",
       " 'LAGUNA',\n",
       " 'LAS MUSAS',\n",
       " 'LAS ROSAS',\n",
       " 'LAS SUERTES',\n",
       " 'LAS TABLAS',\n",
       " 'LAVAPIES',\n",
       " 'LEGANES CENTRAL',\n",
       " 'LEGAZPI',\n",
       " 'LISTA',\n",
       " 'LORANCA',\n",
       " 'LOS ESPARTALES',\n",
       " 'LUCERO',\n",
       " 'MANOTERAS',\n",
       " 'MANUEL BECERRA',\n",
       " 'MANUEL DE FALLA',\n",
       " 'MANUELA MALASAÑA',\n",
       " 'MAR DE CRISTAL',\n",
       " 'MARQUES DE LA VALDAVIA',\n",
       " 'MARQUES DE VADILLO',\n",
       " 'MENDEZ ALVARO',\n",
       " 'MENENDEZ PELAYO',\n",
       " 'MIGUEL HERNANDEZ',\n",
       " 'MIRASIERRA',\n",
       " 'MONCLOA',\n",
       " 'MONTECARMELO',\n",
       " 'MOSTOLES CENTRAL',\n",
       " 'NOVICIADO',\n",
       " 'NUEVA NUMANCIA',\n",
       " 'NUEVOS MINISTERIOS',\n",
       " 'NUÑEZ DE BALBOA',\n",
       " 'ODONNELL',\n",
       " 'OPAÑEL',\n",
       " 'OPERA',\n",
       " 'OPORTO',\n",
       " 'PACIFICO',\n",
       " 'PACO DE LUCIA',\n",
       " 'PALOS DE LA FRONTERA',\n",
       " 'PAN BENDITO',\n",
       " 'PARQUE DE LAS AVENIDAS',\n",
       " 'PARQUE DE LOS ESTADOS',\n",
       " 'PARQUE DE SANTA MARIA',\n",
       " 'PARQUE EUROPA',\n",
       " 'PARQUE LISBOA',\n",
       " 'PARQUE OESTE',\n",
       " 'PAVONES',\n",
       " 'PEÑAGRANDE',\n",
       " 'PINAR DE CHAMARTIN',\n",
       " 'PINAR DEL REY',\n",
       " 'PIO XII',\n",
       " 'PIRAMIDES',\n",
       " 'PITIS',\n",
       " 'PLAZA DE CASTILLA',\n",
       " 'PLAZA DE ESPAÑA',\n",
       " 'PLAZA ELIPTICA',\n",
       " 'PORTAZGO',\n",
       " 'PRADILLO',\n",
       " 'PRINCIPE DE VERGARA',\n",
       " 'PRINCIPE PIO',\n",
       " 'PROSPERIDAD',\n",
       " 'PUEBLO NUEVO',\n",
       " 'PUENTE DE VALLECAS',\n",
       " 'PUERTA DE ARGANDA',\n",
       " 'PUERTA DE TOLEDO',\n",
       " 'PUERTA DEL ANGEL',\n",
       " 'PUERTA DEL SUR',\n",
       " 'QUEVEDO',\n",
       " 'QUINTANA',\n",
       " 'REPUBLICA ARGENTINA',\n",
       " 'RETIRO',\n",
       " 'REYES CATOLICOS',\n",
       " 'RIOS ROSAS',\n",
       " 'RIVAS FUTURA',\n",
       " 'RIVAS URBANIZACIONES',\n",
       " 'RIVAS VACIAMADRID',\n",
       " 'RONDA DE LA COMUNICACION',\n",
       " 'RUBEN DARIO',\n",
       " 'SAINZ DE BARANDA',\n",
       " 'SAN BERNARDO',\n",
       " 'SAN BLAS',\n",
       " 'SAN CIPRIANO',\n",
       " 'SAN CRISTOBAL',\n",
       " 'SAN FERMIN-ORCASUR',\n",
       " 'SAN FERNANDO',\n",
       " 'SAN FRANCISCO',\n",
       " 'SAN LORENZO',\n",
       " 'SAN NICASIO',\n",
       " 'SANTIAGO BERNABEU',\n",
       " 'SANTO DOMINGO',\n",
       " 'SERRANO',\n",
       " 'SEVILLA',\n",
       " 'SIERRA DE GUADALUPE',\n",
       " 'SIMANCAS',\n",
       " 'SOL',\n",
       " 'SUANZES',\n",
       " 'TETUAN',\n",
       " 'TIRSO DE MOLINA',\n",
       " 'TORRE ARIAS',\n",
       " 'TRES OLIVOS',\n",
       " 'TRIBUNAL',\n",
       " 'UNIVERSIDAD REY JUAN CARLOS',\n",
       " 'URGEL',\n",
       " 'USERA',\n",
       " 'VALDEACEDERAS',\n",
       " 'VALDEBERNARDO',\n",
       " 'VALDECARROS',\n",
       " 'VALDEZARZA',\n",
       " 'VELAZQUEZ',\n",
       " 'VENTAS',\n",
       " 'VENTILLA',\n",
       " 'VENTURA RODRIGUEZ',\n",
       " 'VICALVARO',\n",
       " 'VICENTE ALEIXANDRE',\n",
       " 'VILLA DE VALLECAS',\n",
       " 'VILLAVERDE ALTO',\n",
       " 'VILLAVERDE BAJO CRUCE',\n",
       " 'VINATEROS',\n",
       " 'VISTA ALEGRE'}"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnet_weighted.slices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. ---Centrality measures to Madrid data---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(mu_PCI(mnet_no_weighted, ('AVENIDA DE AMERICA', 'metro'), mu = 1))\n",
    "print(mu_PCI(mnet_no_weighted, ('AVENIDA DE AMERICA', 'metro'), mu = 2))\n",
    "\n",
    "# for each node in the mnet_no_weightedwork store the mu_PCI, mlPCI_n, allPCI and lsPCI in a dictionary\n",
    "nl_PCIs = {}\n",
    "for node in mnet_no_weighted.iter_node_layers():\n",
    "    nl_PCIs[node] = {\n",
    "        'mu_PCI': mu_PCI(mnet_no_weighted, node, mu=1),\n",
    "        'mlPCI_n': mlPCI_n(mnet_no_weighted, node),\n",
    "        'allPCI': allPCI(mnet_no_weighted, node),\n",
    "        'lsPCI': lsPCI(mnet_no_weighted, node)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Nodes with lsPCI > 1:\n"
     ]
    }
   ],
   "source": [
    "# show nodes with lsPCI > 1\n",
    "high_lsPCI_nodes = {node: pci['lsPCI'] for node, pci in nl_PCIs.items() if pci['lsPCI'] > 1}\n",
    "print(f\"{len(high_lsPCI_nodes)} Nodes with lsPCI > 1:\")\n",
    "# for node, lsPCI in high_lsPCI_nodes.items():\n",
    "#     print(f\"{node}: {lsPCI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Nodes with allPCI > 1:\n"
     ]
    }
   ],
   "source": [
    "# show nodes with lsPCI > 1\n",
    "high_allPCI_nodes = {node: pci['allPCI'] for node, pci in nl_PCIs.items() if pci['allPCI'] > 1}\n",
    "print(f\"{len(high_allPCI_nodes)} Nodes with allPCI > 1:\")\n",
    "# for node, lsPCI in high_lsPCI_nodes.items():\n",
    "#     print(f\"{node}: {lsPCI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 'interurban', 1: 'urban', 2: 'metro'}]"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, nodes2int, layers2int, int2nodes, int2layers = __get_sparse_tensor__(mnet_no_weighted, return_mappings=True)\n",
    "inv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ILEC_madrid: (3, 242)\n",
      "UEC_madrid: (242,)\n",
      "LHEC_madrid: (3, 242)\n",
      "GHEC_madrid: (3, 242)\n"
     ]
    }
   ],
   "source": [
    "ILEC_madrid, ILEC_int2layers = independent_layer_eigenvector_centrality(mnet_no_weighted)\n",
    "UEC_madrid = uniform_eigenvector_centrality(mnet_no_weighted)\n",
    "LHEC_madrid, LHEC_nodes2int, LHEC_layers2int, LHEC_int2nodes, LHEC_int2layers  = local_heterogeneus_eigenvector_centrality(mnet_no_weighted)\n",
    "GHEC_madrid, GHEC_nodes2int, GHEC_layers2int, GHEC_int2nodes, GHEC_int2layers = global_heterogeneus_eigenvector_centrality(mnet_no_weighted)\n",
    "\n",
    "print(\"ILEC_madrid:\", ILEC_madrid.shape)\n",
    "print(\"UEC_madrid:\", UEC_madrid.shape)\n",
    "print(\"LHEC_madrid:\", LHEC_madrid.shape)\n",
    "print(\"GHEC_madrid:\", GHEC_madrid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2144022e-21, shape=(), dtype=float32)\n",
      "tf.Tensor(0.19444656, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NODE = 'AVENIDA DE AMERICA'\n",
    "NODE = 'PLAZA ELIPTICA'\n",
    "LAYER = 'urban'\n",
    "print(GHEC_madrid[GHEC_layers2int[0][LAYER]][GHEC_nodes2int[NODE]])\n",
    "\n",
    "print(LHEC_madrid[LHEC_layers2int[0][LAYER]][LHEC_nodes2int[NODE]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. --- SIR ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interlayers_beta = {\n",
    "    (('metro',),('metro',)): 65000,\n",
    "    (('metro',), ('urban',)): 4,\n",
    "    (('metro',), ('interurban',)): 0.9,\n",
    "\n",
    "    (('urban',), ('urban',)): 50000,\n",
    "    (('urban',), ('interurban',)): 10000000,\n",
    "    (('urban',), ('metro',)):4,\n",
    "\n",
    "    (('interurban',), ('interurban',)): 50000,\n",
    "    (('interurban',), ('metro',)): 4,\n",
    "\n",
    "    (('interurban',), ('urban',)): 10000000\n",
    "}\n",
    "intra_gamma = {\n",
    "    ('metro',): 0.1,\n",
    "    ('urban',): 0.2,\n",
    "    ('interurban',): 0.2\n",
    "}\n",
    "\n",
    "initial_state = {\n",
    "    node:  0\n",
    "    for node in mnet_weighted.iter_node_layers()\n",
    "}\n",
    "\n",
    "initial_state[('AVENIDA DE AMERICA', 'metro')] = 1  # Infected node\n",
    "initial_state[('PLAZA ELIPTICA', 'urban')] = 1  # Infected node\n",
    "\n",
    "nodeSizeDict = {\n",
    "    (node, layer): 0.01 \n",
    "    for (node, layer) in mnet_no_weighted.iter_node_layers()\n",
    "}\n",
    "\n",
    "final_states = SIR_net_diffusion( mnet_weighted,\n",
    "    interlayers_beta=interlayers_beta,\n",
    "    intra_gamma=intra_gamma,\n",
    "    iterations= 10,\n",
    "    initial_state=initial_state\n",
    ")\n",
    "\n",
    "for i, state in enumerate(final_states):\n",
    "# plot the network labeling only the starting infected nodes, all infected nodes are in red\n",
    "    fig = plt.figure(figsize=(36, 24))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    nodeColorDict = {\n",
    "        (node, layer): 'red' if state[(node, layer)] == 1 else 'blue' if state[(node, layer)] == 2 else 'gray'\n",
    "        for (node, layer) in mnet_weighted.iter_node_layers()\n",
    "    }\n",
    "    #initial_state eq 1\n",
    "    nodeLabelDict = {\n",
    "        (node, layer): node if initial_state[(node, layer)] == 1 else ''\n",
    "        for (node, layer) in mnet_weighted.iter_node_layers()\n",
    "    }\n",
    "    draw(\n",
    "        mnet_weighted,\n",
    "        nodeCoords=node_coords,\n",
    "        nodeColorDict=nodeColorDict,\n",
    "        nodeLabelDict=nodeLabelDict,       # Show only selected labels\n",
    "        nodeLabelRule={},                  # No automatic labels\n",
    "        defaultNodeLabel=\"\",               # Hide labels\n",
    "        defaultEdgeAlpha=0.4,\n",
    "        nodeSizeDict=nodeSizeDict, # Scaled by community size\n",
    "        layergap=1,\n",
    "        ax=ax\n",
    "    )\n",
    "    # print('infected nodes:', [node for node, state in state.items() if state == 1])\n",
    "    # print('recovered nodes:', [node for node, state in state.items() if state == 2])\n",
    "\n",
    "    plt.title(f\"State after {i} iterations\")\n",
    "    plt.savefig(f'imgs/SIR/sir_state_{i}.png', dpi=300, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNVisual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
